<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 闲谈机器学习(4) · ClT's Blog</title><meta name="description" content="闲谈机器学习(4) - ClT"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/foowaa" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about/index.html" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">闲谈机器学习(4)</h1><div class="post-time">Apr 9, 2016</div><div class="post-content"><h2 id="1-kernel-Ridge-Regression"><a href="#1-kernel-Ridge-Regression" class="headerlink" title="1.kernel Ridge Regression"></a>1.kernel Ridge Regression</h2><p>将表示定理用于Ridge Regression，可以将线性Ridge Regression推广到非线性。再叙述一次表示定理：</p>
<p>简单来说，一个优化问题表示为：argmin err(out)=l[(x1,y1,f(x1),…,(xn,yn,f(xn)]+g(||f||) </p>
<p>此时，l(.)非负，g(.)为单调增函数，那么存在满足Mercer定理的核函数，使得解总可以写作：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.6.5.gif"></p>
<p>对于l2-norm regularization LM：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.7.1.gif"><br>我们将看到，这个定理的强大作用。它使LM变成了非线性模型，而不需要对LM的框架更改许多。</p>
<p>Ridge Reg可以表示为:argmin 平方损失+l2-norm regularization。完全符合表示定理。则：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.7.2.gif"></p>
<p>使用矩阵表示：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.7.3.gif"></p>
<p>解得</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.7.4.gif"><br>但是这个矩阵比较dense，数值解会浪费时间，且解不稳定。所以实务中用的不多。</p>
<h2 id="2-kernel-l2-norm-regularized-LogReg"><a href="#2-kernel-l2-norm-regularized-LogReg" class="headerlink" title="2.kernel l2-norm regularized LogReg"></a>2.kernel l2-norm regularized LogReg</h2><p>将表示定理用于l2-norm regularized LogReg，可以将线性Ridge Regression推广到非线性。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.7.5.gif"></p>
<p>这个问题不像SVM具有稀疏性，实务中用的也不多。</p>
<h2 id="3-Probabilistic-SVM"><a href="#3-Probabilistic-SVM" class="headerlink" title="3.Probabilistic SVM"></a>3.Probabilistic SVM</h2><p>继续研究LogReg和SVM。</p>
<p>soft-margin SVM使用hinge loss，LogReg使用log loss。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.7.6.jpg" width="500" height="200"></p>
<p>可以看到log-loss和hinge-loss很相近，实际上SVM约等于l2-norm regularized LogReg。通常情况下，他们的性能也相当。LogReg主要优势在于可以直接解释为概率。SVM的主要优势在于SVM的解具有稀疏性，因而需要的样本也更少，开销小。</p>
<p>我们可以将SVM和LogReg结合起来。先使用SVM得到一个线性模型。然后将转换后的数据带入LogReg，SVM中可以使用kernel，这个模型同样可以实现非线性分类。这就是所谓的2-stage model。叫做probabilistic SVM。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.7.7.gif"></p>
<h2 id="4-SVR"><a href="#4-SVR" class="headerlink" title="4.SVR"></a>4.SVR</h2><p>将SVM的思想推广到回归就得到了SVR。让回归曲线也尽可能“胖”一点。仿照soft-margin SVM：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.9.1.gif"></p>
<p>按照SVM的思路，可以导出SVR，同样可以kernel化。</p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/04/10/info-theory/" class="prev">上一篇</a><a href="/2016/04/09/ML3/" class="next">下一篇</a></div><div data-thread-key="2016/04/09/ML4/" data-title="闲谈机器学习(4)" data-url="http://yoursite.com/2016/04/09/ML4/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"seansun"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2016 <a href="http://yoursite.com">ClT</a>.This simple Blog is created by Hexo.</p></div></footer><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>