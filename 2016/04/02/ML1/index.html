<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 闲谈机器学习(1) · ClT's Blog</title><meta name="description" content="闲谈机器学习(1) - ClT"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/foowaa" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about/index.html" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">闲谈机器学习(1)</h1><div class="post-time">Apr 2, 2016</div><div class="post-content"><h2 id="1-OLS：一般最小二乘回归"><a href="#1-OLS：一般最小二乘回归" class="headerlink" title="1.OLS：一般最小二乘回归"></a>1.OLS：一般最小二乘回归</h2><p>这是最普通也是最原始的回归模型。在20世纪就由Gauss等数学家提出。OLS的最优特点是有闭式解，甚至不需要太多的优化手段。OLS顾名思义，采用平方损失函数。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.1.gif"></p>
<p>那么最优化问题为：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.2.gif"></p>
<p>得到：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.3.gif"></p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.4.gif"></p>
<p>由于有close form的解（不讨论广义逆等情况），所以求解起来非常容易。请注意w*，之后的很多变形和它都有关系。</p>
<h2 id="2-Ridge-Regression"><a href="#2-Ridge-Regression" class="headerlink" title="2.Ridge Regression"></a>2.Ridge Regression</h2><p>对于预测的w*，有如下定理：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.5.gif">，这是bias-varience分解的另一种表达。</p>
<p>由于w是无偏估计（Gauss-Markov），所以：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.6.gif"></p>
<p>当X’X的特征值非常小时，会出现MSE非常大的情况。所以我们期望对w*的表达式做一改造，使之避免这种情况。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.7.gif"></p>
<p>显然，X’X的特征值将同时增大。这也是防止X’X变为奇异矩阵的方法。这会减小varience。</p>
<p>从机器学习的角度来说，这相当于Regul.，惩罚过大的协方差。使用l2-norm Regul.(Tikhonov Regul.)防止overfitting。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.8.png"></p>
<p>Ridge Reg的实现和OLS很相似，这里不再赘述。只有一个超参数需要人为调整，可以采用Grid Search或者使用先验知识等方法进行设定。</p>
<h2 id="3-Lasso"><a href="#3-Lasso" class="headerlink" title="3.Lasso"></a>3.Lasso</h2><p>我们还可以发现，Ridge Reg解决的实际上是数据的特征&gt;数据量(n&gt;m)的问题，由于r(X’X)&lt;=r(X)&lt;=m&lt;n，而X’X是n*n的矩阵。所以减少数据的维数就可以解决。</p>
<p>Lasso添加l1-norm Regul.罚项，它是一种更趋向于选择稀疏解的线性回归。（具体可以看统计学教材）</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.9.png"></p>
<p>罚函数为P(w)，我们假设P(w)为向量lp范。p&gt;1，此时P(w)为光滑凸函数；p=1，为不光滑凸函数；0&lt;p&lt;1为非凸函数.</p>
<p>我们倾向于解决p&gt;1的可微凸优化问题。当p=1时，就得到Lasso。p=2为Ridge Regression。Lasso无法给出一个闭式解。需要采用优化方法。由于Lasso的稀疏性，广泛运用在sparse learning中</p>
<h2 id="4-概率视角"><a href="#4-概率视角" class="headerlink" title="4.概率视角"></a>4.概率视角</h2><p>以上的理论推导并未用到任何概率知识，仅仅采用最小二乘法，解了一个线性方程组而已。以下采用概率统计的视角的得到相同的结果。</p>
<p>显然，</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.24.1.gif"></p>
<p>可以看出，OLS实际是在Gaussian分布下的MLE。</p>
<p>我们再进行一些更复杂的推导，可以得出另一个结论。选用共轭先验：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.24.2.gif"></p>
<p>Ridge Reg实际是先验为Gaussian的MAP估计。</p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/04/02/ML2/" class="prev">PRVE</a></div><div data-thread-key="2016/04/02/ML1/" data-title="闲谈机器学习(1)" data-url="http://yoursite.com/2016/04/02/ML1/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"seansun"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2016 <a href="http://yoursite.com">ClT</a>, unless otherwise noted.</p></div></footer><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>