<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Machine Learning(1) · ClT's Blog</title><meta name="description" content="Machine Learning(1) - ClT"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/foowaa" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about.html" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Machine Learning(1)</h1><div class="post-time">Apr 2, 2016</div><div class="post-content"><h1 id="机器学习1——线性模型-1"><a href="#机器学习1——线性模型-1" class="headerlink" title="机器学习1——线性模型(1)"></a>机器学习1——线性模型(1)</h1><h2 id="1-为什么选择线性模型？"><a href="#1-为什么选择线性模型？" class="headerlink" title="1.为什么选择线性模型？"></a>1.为什么选择线性模型？</h2><ul>
<li>线性模型(LM)简单。线性模型的参数较少，直观，易于优化计算。</li>
<li>抗overfitting的效果好。较少的参数意味着模型的泛化能力很强，可以有效的防止overfitting。符合Occam’s Razor原理。</li>
<li>数学基础坚实。基于统计学和优化计算。</li>
<li>扩展能力强。一般的线性模型可以轻易的扩展为GLM，ridge LinReg（岭回归），Lasso，SVM。由表示定理，可以kernel化，进而进行非线性分类和回归。继续扩展，还有Bayesian LinReg和Bayesian LogReg和Gaussian Processes回归等。可以说LM是机器学习领域涵盖面非常广的一大类“好”模型。实用且高效。</li>
<li>LM主要包括：回归(Reg)模型和SVM模型。</li>
</ul>
<h2 id="2-OLS：一般最小二乘回归"><a href="#2-OLS：一般最小二乘回归" class="headerlink" title="2.OLS：一般最小二乘回归"></a>2.OLS：一般最小二乘回归</h2><p>这是最普通也是最原始的回归模型。在20世纪就由Gauss等数学家提出。OLS的最优特点是有闭式解，甚至不需要太多的优化手段。OLS顾名思义，采用平方损失函数。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.1.gif"></p>
<p>那么最优化问题为：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.2.gif"></p>
<p>得到：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.3.gif"></p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.4.gif"></p>
<p>由于有close form的解（不讨论广义逆等情况），所以求解起来非常容易。请注意w*，之后的很多变形和它都有关系。</p>
<h2 id="3-Ridge-Regression"><a href="#3-Ridge-Regression" class="headerlink" title="3.Ridge Regression"></a>3.Ridge Regression</h2><p>对于预测的w*，有如下定理：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.5.gif">，这是bias-varience分解的另一种表达。</p>
<p>由于w是无偏估计（Gauss-Markov），所以：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.6.gif"></p>
<p>当X’X的特征值非常小时，会出现MSE非常大的情况。所以我们期望对w*的表达式做一改造，使之避免这种情况。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.7.gif"></p>
<p>显然，X’X的特征值将同时增大。这也是防止X’X变为奇异矩阵的方法。这会减小varience。</p>
<p>从机器学习的角度来说，这相当于regularization，惩罚过大的协方差。使用l2-norms(Tikhonov Regularization)防止overfitting。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.8.png"></p>
<p>Ridge Reg的实现和OLS很相似，这里不再赘述。只有一个超参数需要人为调整，可以采用Grid Search或者使用先验知识等方法进行设定。</p>
<h2 id="4-Lasso"><a href="#4-Lasso" class="headerlink" title="4.Lasso"></a>4.Lasso</h2><p>我们还可以发现，Ridge Reg解决的实际上是数据的特征&gt;数据量(n&gt;m)的问题，由于r(X’X)&lt;=r(X)&lt;=m&lt;n，而X’X是n*n的矩阵。所以减少数据的维数就可以解决。</p>
<p>Lasso是一种更趋向于选择稀疏解的线性回归。（具体可以看统计学教材）</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.23.9.png"></p>
<p>罚函数为P(w)，我们假设P(w)为向量lp范。p&gt;1，此时P(w)为光滑凸函数；p=1，为不光滑凸函数；0&lt;p&lt;1为非凸函数.</p>
<p>我们倾向于解决p&gt;1的可微凸优化问题。当p=1时，就得到Lasso。p=2为Ridge Regression。Lasso无法给出一个闭式解。需要采用优化方法。（具体的优化手段我并不清楚）。Lasso的思想已经延伸到稀疏学习、压缩感知等领域。</p>
<p>和Lasso相似的还有逐步回归，采用Greedy策略，也可以达到一定意义上的稀疏解的效果。这些方法和特征选择以及降维都有所联系。</p>
<h2 id="5-Baysian-LinReg"><a href="#5-Baysian-LinReg" class="headerlink" title="5.Baysian LinReg"></a>5.Baysian LinReg</h2><p>Bayesian LinReg是将线性回归放在贝叶斯统计的语义下推出的一种回归。看起来OLS的推理并没有用到统计，实际上，可以看到y服从高斯分布。前面最小平方损失的优化过程实际也可以看做是最大似然（MLE）估计参数的过程。</p>
<p>在贝叶斯语义下，我们假设参数分为2种：参数parameter和超参数hyperparameter。其中parameter是随机变量(r.v.)，hyperparameter是一般的变量（或者是uninformative prior)。parameter的prior分布p.d.f.参数为hyperparameter。观察得到的是likelihood。通过全概率和贝叶斯公式，我们可以得到posterior分布（参数的后验分布）。我们需要做的是使用type-II MLE估计参数（或者使用其他方法得到参数后验的值）。由于Bayesian Occam’s Razor原理，Bayesian LinReg会有更好的泛化能力。</p>
<p>先验的一般选择或者共轭先验(conjugate prior)，我们需要注意的是evidence服从正态，协方差不予考虑，此时参数的的先验也是正态。（形式化表达参见MLPP 7.6）</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.1.2.png"></p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/04/02/ML2/" class="prev">PRVE</a></div><div data-thread-key="2016/04/02/ML1/" data-title="Machine Learning(1)" data-url="http://yoursite.com/2016/04/02/ML1/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"seansun"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2016 <a href="http://yoursite.com">ClT</a>, unless otherwise noted.</p></div></footer><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>