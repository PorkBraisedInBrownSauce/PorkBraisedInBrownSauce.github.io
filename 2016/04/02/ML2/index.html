<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 闲谈机器学习(2) · ClT's Blog</title><meta name="description" content="闲谈机器学习(2) - ClT"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/foowaa" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about/index.html" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">闲谈机器学习(2)</h1><div class="post-time">Apr 2, 2016</div><div class="post-content"><h2 id="1-简单的线性分类模型"><a href="#1-简单的线性分类模型" class="headerlink" title="1.简单的线性分类模型"></a>1.简单的线性分类模型</h2><p>前面介绍的线性模型(LM)适应于回归。一般的线性模型可以用于分类吗？可以有以下朴素模型：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.31.1.gif"><br>但是这个优化问题不可导，类似于组合优化，是一个NP-hard问题。</p>
<p>放宽一下，不要求那么精确，我们只要看y(w’x)是否同号即可：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.31.2.gif"><br>这被称为：感知机模型(perceptron)，是最简单的一个线性分类模型，也是最简单的神经网络模型(ANN)。这个问题容易优化，使用SGD（随机梯度下降法）很容易求解。但是这个模型能力非常有限，具体将在ANN部分说明。</p>
<h2 id="2-广义线性模型-GLM"><a href="#2-广义线性模型-GLM" class="headerlink" title="2.广义线性模型(GLM)"></a>2.广义线性模型(GLM)</h2><p>GLM是LM的推广，在统计学上有很严密的理论支撑。机器学习领域最明显的应用是：logistic回归。<br>这里通过3个角度简单说明LogReg。</p>
<p>所谓GLM，是指它扩展了OLS的误差正态性假设，假设y服从指数分布族的一个分布（正态分布是指数分布族的一支）。如果y~Bern(p)(y服从伯努利分布），那么：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.31.3.gif"><br>当p&lt;=0.5，则y=-1;当p&gt;0.5，则y=+1.<br>这个模型具有很强的扩展性，如果y~Cat(p1,p2,…)时，可以得到用于多分类的LogReg。（这里的具体推导不书写）</p>
<p>第二个角度更容易理解。我们定义归一化sigmoid函数(softmax)：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.31.4.gif"><br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.31.5.png" width="500" height="200"><br>sigmoid(w’x)相当于将值域压缩到[0,1]。</p>
<p>并且，sigmoid函数二阶可导，g’(z)=g(z)(1-g(z))，这对优化而言是好事。对LogReg的优化采用MLE+GD/SGD/Newton/quasi-Newton等数值优化方法皆可。使用GD或者Newton法都可以迭代求解。<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.1.1.gif"><br>第三个角度仍然从形式化的LM出发。</p>
<p>还是从模型本身出发，P(Y=0|X)=1/(1+exp(-y(w’x))，MLE的目的是让P(Y=1|X)最大，也就是P(Y=0|X)最小：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.19.2.gif"></p>
<p>erri=log(1+exp(-ys))，而log(1+exp(-ys))被称为log-loss。</p>
<p>如果y~N(0,1)，我们将得到probit模型，这是一个和LogReg很相似的模型，同样具有归一化作用。</p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/04/09/ML3/" class="prev">PRVE</a><a href="/2016/04/02/ML1/" class="next">NEXT</a></div><div data-thread-key="2016/04/02/ML2/" data-title="闲谈机器学习(2)" data-url="http://yoursite.com/2016/04/02/ML2/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"seansun"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2016 <a href="http://yoursite.com">ClT</a>.This simple Blog is created by Hexo.</p></div></footer><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>