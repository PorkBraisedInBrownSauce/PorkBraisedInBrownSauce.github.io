<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Machine Learning(2) · ClT's Blog</title><meta name="description" content="Machine Learning(2) - ClT"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/foowaa" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about.html" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Machine Learning(2)</h1><div class="post-time">Apr 2, 2016</div><div class="post-content"><h1 id="机器学习1——线性模型-2"><a href="#机器学习1——线性模型-2" class="headerlink" title="机器学习1——线性模型(2)"></a>机器学习1——线性模型(2)</h1><h2 id="1-形式化的LM"><a href="#1-形式化的LM" class="headerlink" title="1.形式化的LM"></a>1.形式化的LM</h2><ul>
<li>得分：s=w’x。表示training set中不同属性的加权求和。feature space-&gt;实数空间的映射。</li>
<li>学习目标：argmin err(out)=err(in)+A=f(s,y)+A。学习目标是最小化输出误差，输出误差=经验误差+泛化能力损失=f(输出，得分）+泛化能力损失</li>
<li>对OLS来说，采用平方误差，A=0；对Ridge Reg来说，采用平方误差，A=Tikhonov Regularization；对Lasso来说，采用平方误差，A=l1-norm。</li>
</ul>
<h2 id="2-简单的线性分类模型"><a href="#2-简单的线性分类模型" class="headerlink" title="2.简单的线性分类模型"></a>2.简单的线性分类模型</h2><p>前面介绍的线性模型(LM)适应于回归。一般的线性模型可以用于分类吗？可以有以下朴素模型：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.31.1.gif"><br>但是这个优化问题不可导，类似于组合优化，是一个NP-hard问题。</p>
<p>放宽一下，不要求那么精确，我们只要看y(w’x)是否同号即可：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.31.2.gif"><br>这被称为：感知机模型(perceptron)，是最简单的一个线性分类模型，也是最简单的神经网络模型(ANN)。这个问题容易优化，使用SGD（随机梯度下降法）很容易求解。但是这个模型能力非常有限，具体将在ANN部分说明。</p>
<h2 id="3-广义线性模型-GLM"><a href="#3-广义线性模型-GLM" class="headerlink" title="3.广义线性模型(GLM)"></a>3.广义线性模型(GLM)</h2><p>GLM是LM的推广，在统计学上有很严密的理论支撑。机器学习领域最明显的应用是：logistic回归。<br>这里通过3个角度简单说明LogReg。</p>
<p>所谓GLM，是指它扩展了OLS的误差正态性假设，假设y服从指数分布族的一个分布（正态分布是指数分布族的一支）。如果y~Bern(p)(y服从伯努利分布），那么：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.31.3.gif"><br>当p&lt;=0.5，则y=-1;当p&gt;0.5，则y=+1.<br>这个模型具有很强的扩展性，如果y~Cat(p1,p2,…)时，可以得到用于多分类的LogReg。（这里的具体推导不书写）</p>
<p>第二个角度更容易理解。我们定义归一化sigmoid函数(softmax)：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.31.4.gif"><br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/3.31.5.png" width="500" height="200"><br>sigmoid(w’x)相当于将值域压缩到[0,1]。</p>
<p>并且，sigmoid函数二阶可导，g’(z)=g(z)(1-g(z))，这对优化而言是好事。对LogReg的优化采用MLE+GD/SGD/Newton/quasi-Newton等数值优化方法皆可。使用GD或者Newton法都可以迭代求解。<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.1.1.gif"><br>第三个角度仍然从形式化的LM出发。</p>
<p>从L(w)可以得出：err(in)=log(1+exp(s))-s<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.1.4.gif"><br>且<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.1.5.gif"><br>可以认为log(1+exp(s))-s=log(1+exp(-s))，而log(1+exp(-s))被称为log-loss。还有一种Loss叫做cross-entropy-loss，在LogReg中，log-loss=cross-entopy-loss。所以LogReg的err(in)=log-loss(s)=cross-entropy-loss(s)</p>
<p>如果y~N(0,1)，我们将得到probit模型，这是一个和LogReg很相似的模型，同样具有归一化作用。</p>
<h2 id="4-Bayesian-LogReg"><a href="#4-Bayesian-LogReg" class="headerlink" title="4.Bayesian LogReg"></a>4.Bayesian LogReg</h2><p>y~Bern(p)，我们找不到一个优良的先验分布，但又不想使用uninformative prior。这个时候使用一种新的技巧：appropriation来做。由于Gaussian有良好的congjugate prior，Bern和Gaussian都属于指数分布族，我们考虑使用Gaussian去近似Bern分布。这个方法叫Laplace appropriation。<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.1.3.gif"><br>具体不再深入。（可见PRML和MLPP）</p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/04/02/ML1/" class="next">下一篇</a></div><div data-thread-key="2016/04/02/ML2/" data-title="Machine Learning(2)" data-url="http://yoursite.com/2016/04/02/ML2/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"seansun"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2016 <a href="http://yoursite.com">ClT</a>, unless otherwise noted.</p></div></footer><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>