<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 闲谈机器学习(8) · ClT's Blog</title><meta name="description" content="闲谈机器学习(8) - ClT"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/foowaa" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/public/about/index.html" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">闲谈机器学习(8)</h1><div class="post-time">Apr 13, 2016</div><div class="post-content"><p>决策树(D.T.)属于conditional voting的multi-stage模型。D.T.的base learner是只含2层的D.T.，叫做decision stump，每一层在一定条件下进行分支。</p>
<p>同样套入统计机器学习框架，我们关注的还是erri和Regul.两个问题。（当然还有优化方法和数值计算问题）</p>
<h2 id="1-Regression-Tree"><a href="#1-Regression-Tree" class="headerlink" title="1.Regression Tree"></a>1.Regression Tree</h2><p>首先看erri。对于回归，最普遍的是采用二次损失。在D.T.中，将erri解释为impurity，在树分支的过程中，impurity要越来越低，而每一步的分支要尽可能降低impurity。</p>
<p>每一步要寻找让impurity降低最明显的feature，并且要找到这个feature让impurity降低最明显的阈值。</p>
<p>如下例：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.13.3.png"></p>
<p>在二维空间，表示的分界面如下：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.13.4.png"></p>
<p>可以看到，RegTree实际上在用一定数量的超平面模拟分界曲面，如果分支无限下去，就可以得到曲面。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.13.5.png"></p>
<h2 id="2-Classification-Tree"><a href="#2-Classification-Tree" class="headerlink" title="2.Classification Tree"></a>2.Classification Tree</h2><p>对于分类，我们前面采用过0/1 loss,log-loss,cross-entropy loss,exp-loss,hindge loss,etc。这里没有得分的概念，输入空间是完全离散的，但是impurity依然适用。可以采用误分类率，信息增益，Gini index等。CART采用Gini index。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.13.6.gif"></p>
<p>直观上说，Gini反映了数据集D中随机抽取两个样本，其类别标记不一致的概率，也就是impurity。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.13.7.png"></p>
<h2 id="3-pruning"><a href="#3-pruning" class="headerlink" title="3.pruning"></a>3.pruning</h2><p>不论是RegTree还是ClsTree，共有的缺点是variance较大，对外部反应过于敏感，泛化能力不足。容易overfitting。这时肯定要采用Regul.，对D.T来说，就是pruning。</p>
<p>prepruning是在D.T.的生长过程中，对每个节点在划分之前，先于val. set上的数据对比，看正确率大小，如果在val. set上正确率提升，就划分，否则就禁止划分。postpruning是在训练结束后回溯地进行pruning。prepruning不仅能防止overfitting，还可以减少训练时间。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.13.8.png"></p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/04/14/ML9/" class="prev">PRVE</a><a href="/2016/04/12/ML7/" class="next">NEXT</a></div><div data-thread-key="2016/04/13/ML8/" data-title="闲谈机器学习(8)" data-url="http://yoursite.com/2016/04/13/ML8/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"seansun"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2016 <a href="http://yoursite.com">ClT</a>, unless otherwise noted.</p></div></footer><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>