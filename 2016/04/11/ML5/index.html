<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 闲谈机器学习(5) · ClT's Blog</title><meta name="description" content="闲谈机器学习(5) - ClT"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/foowaa" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about/index.html" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">闲谈机器学习(5)</h1><div class="post-time">Apr 11, 2016</div><div class="post-content"><h2 id="1-Transform-OR-Basis"><a href="#1-Transform-OR-Basis" class="headerlink" title="1.Transform OR Basis"></a>1.Transform OR Basis</h2><p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.24.3.gif"></p>
<p>对x在一个basis下施以变换，使得x首先完成一个特征转换。LM是指w的线性，而不是特征特征一定是线性。</p>
<h2 id="2-kernel-trick"><a href="#2-kernel-trick" class="headerlink" title="2.kernel trick"></a>2.kernel trick</h2><p>我采用MLPP的说法：</p>
<blockquote>
<p>Rather than defining our feature vector in terms of kernels, φ(x) = [κ(x,x 1),…,κ(x,xn)], we can instead work with the original feature vectors x, but modify the algorithm so that it replaces all inner products of the form  &lt;x,x’&gt; with a call to the kernel function, κ(x,x’).</p>
</blockquote>
<p>凡是在内积空间的变换总是可以有kernel与之对应。而且如前所述，kernel的一大特点是可以embedding infinite features。Representer Theorem只是kernel trick一个子集。</p>
<h2 id="3-kernel-kNN-and-kernel-k-means"><a href="#3-kernel-kNN-and-kernel-k-means" class="headerlink" title="3.kernel kNN and kernel k-means"></a>3.kernel kNN and kernel k-means</h2><p>kNN天然可以kernelized，因为采用l2-norm的kNN天然具备内积这一个优化条件。</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.24.4.gif"></p>
<p>l2-norm的k-means也是这样。两点之间的距离可以化为kernel。</p>
<h2 id="4-kernel-machine"><a href="#4-kernel-machine" class="headerlink" title="4.kernel machine"></a>4.kernel machine</h2><p>GLM中feature表示为：<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.12.1.gif">叫做kernel machine。</p>
<p>如果为RBF kernel，则得到一个RBF NNet。误差采用二次误差，激活函数采用RBF函数。<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.12.2.png"><br>Architecture of a radial basis function network. An input vector x is used as input to all radial basis functions, each with different parameters. The output of the network is a linear combination of the outputs from radial basis functions.</p>
<center><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.12.3.png"></center>

<center><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.12.4.png"></center>

<p>注意，这和RBF-kernelized SVM很相似，除了采用的loss func.不同，实际上，ANN可以包含一切supervised model，ANN每一层都在做feature transformation~T(x)，BP算法在argmin erri。LM只是层数较少的（2~3层）ANN而已。</p>
<p>再注意到，如果采用uniform vote，那么这个kernel machine等同于欧氏距离的kNN算法。因为内积空间的内积度量实际也在距离度量，RBF kernel实际在度量特征的欧氏距离。</p>
<p>如何确定ci？可以使用clustering，例如k-means。</p>
<p>如果我不想使用unsupervised算法呢？最直接的方法是：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.12.5.gif"></p>
<p>但是这样会导致模型的泛化能力很成问题，计算复杂度也有点高，我们希望w尽可能稀疏一点，这样就引入了sparse vector machine，SVM就是一种sparse vector machine。</p>
<h2 id="5-kernel-PCA"><a href="#5-kernel-PCA" class="headerlink" title="5.kernel PCA"></a>5.kernel PCA</h2><p>PCA方法实际是在操作Gram矩阵，而Gram矩阵与kernel有天然的联系。所以kernel PCA是最天然不过的了。对于非线性相关的数据，通过kernel将其转化为线性相关问题，这样PCA就可以奏效。<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.12.6.png"></p>
<p>则kernel为：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.12.7.png"></p>
<p>这个kernel可以组成kernel矩阵，也是cov矩阵，再做一点变换：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.12.8.gif"></p>
<p>对变换后的kernel矩阵做标准的PCA操作就可以得到kernel PCA</p>
<h2 id="6-非参统计的核方法（待）"><a href="#6-非参统计的核方法（待）" class="headerlink" title="6.非参统计的核方法（待）"></a>6.非参统计的核方法（待）</h2></div></article></div></section><footer><div class="paginator"><a href="/2016/04/12/ML6/" class="prev">上一篇</a><a href="/2016/04/10/info-theory/" class="next">下一篇</a></div><div data-thread-key="2016/04/11/ML5/" data-title="闲谈机器学习(5)" data-url="http://yoursite.com/2016/04/11/ML5/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"seansun"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2016 <a href="http://yoursite.com">ClT</a>.This simple Blog is created by Hexo.</p></div></footer><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>