<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 闲谈机器学习(12) · ClT's Blog</title><meta name="description" content="闲谈机器学习(12) - ClT"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/foowaa" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about/index.html" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">闲谈机器学习(12)</h1><div class="post-time">May 4, 2016</div><div class="post-content"><p>Naive Bayes是著名的机器学习领域的十大著名算法之一，也是一种高效的生成式算法。Naive Bayes基于一个重要假设：条件独立性。我们说过，这是一个生成式算法，所以必然要对联合分布进行建模，但是由于得到一个类别的feature有很多，如果使用概率的乘法公式，计算确实复杂，并且不同feature之间的相关性我们也不清楚，那么姑且认为，同一类别的前提下，不同feature之间相互独立：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.1.png"></p>
<p>得到了likelihood，假设p(x|y)服从伯努利分布，先验p(y)服从分类分布。那么，</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.2.png"></p>
<p>得到MLE的结果，</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.3.png"></p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.4.png"></p>
<p>OK，那么naive Bayes也就得到了。这里需要注意，条件独立性的重要作用，不同的条件独立性假设将得到不同的概率图模型（PGM），Naive Bayes也是一种简单的PGM。</p>
<h3 id="Bayesian框架"><a href="#Bayesian框架" class="headerlink" title="Bayesian框架"></a>Bayesian框架</h3><p>这部分又将进入贝叶斯统计的框架。首先确定先验，还是使用共轭先验：<br><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.6.png"></p>
<p>仿照Naive Bayes，写出predictive:</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.5.gif"></p>
<p>注意到，前一部分是Dir-Cat model，后一部分是Beta-Bern model。使用这两个model的posterior mean替换参数：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.7.png"></p>
<p>这也就是所谓的Bayesian Naive Bayes。</p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/05/04/ML13/" class="prev">PRVE</a><a href="/2016/04/20/ML11/" class="next">NEXT</a></div><div data-thread-key="2016/05/04/ML12/" data-title="闲谈机器学习(12)" data-url="http://yoursite.com/2016/05/04/ML12/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"seansun"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2016 <a href="http://yoursite.com">ClT</a>, unless otherwise noted.</p></div></footer><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>