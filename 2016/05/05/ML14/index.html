<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 闲谈机器学习(14) · ClT's Blog</title><meta name="description" content="闲谈机器学习(14) - ClT"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/foowaa" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about.html" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">闲谈机器学习(14)</h1><div class="post-time">May 5, 2016</div><div class="post-content"><p>隐变量模型(LVM)也是机器学习中一种常见模型，隐变量(LV)是observed var.的反义词。从广义角度来说，所谓学习不也就是获得这些隐变量吗？</p>
<p>狭义而言，LVM主要包括：</p>
<ul>
<li>mixture model</li>
<li>factor analysis</li>
<li>PCA（主成分分析）</li>
<li>ICA（独立成分分析）</li>
<li>LDA</li>
</ul>
<p>mixture model的变量为连续分布，隐变量为离散分布；因子分析则都是连续分布；PCA基于特征值分解；独立成分分析则是信号处理的内容。</p>
<p>EM算法是估计LVM中参数的主要算法。下面主要说EM算法的思想。</p>
<p>设X为observed var.，Z为LV，\theta为参数。因为我们只观察到了X，使用MLE，先来看marginal likelihood：</p>
<p><img src="https://upload.wikimedia.org/math/5/e/4/5e48c3658bb2893d425a52e4d31bde3b.png"></p>
<p>现在最大化这个函数就可以得到\theta的值了。麻烦的是Z和\theta都是未知的，这就很难求了。</p>
<p>从直观的角度来思考，如果隐变量z的分布和观察到x之后“猜测”z的分布应该尽可能接近。即期望：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/5.5.1.gif"></p>
<p>q表示z服从的分布。</p>
<p>构造如下式子：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/5.5.2.gif"></p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/5.5.3.gif"></p>
<p>需要最大化这个式子：</p>
<p><img src="http://7xs6jl.com1.z0.glb.clouddn.com/5.5.4.gif"></p>
<p>发现没有，EM算法前一部分已经出现。我们只管这部分，采用2阶段优化，就会得到一个近似解。</p>
<p>具体步骤如下：</p>
<ol>
<li>初始化</li>
<li>迭代直到收敛</li>
<li>计算log p(x,z|\theta)，这里面既有变量，也有隐变量，还有参数。由于EM算法尤其适用于指数分布族，所以log之后形式会比较简单。</li>
<li>E-step：对log p(x,z|\theta)关于z求期望，其中z用z的估计值代替，z的估计值由上一步的\theta求得。</li>
<li>M-step：最大化上式，求得\theta。转至第2步</li>
</ol>
<p>mixture Gaussian是典型的LVM模型，observed var.约定为高斯分布，LV约定为Cat分布。其中待估参数有3大类。</p>
<p>可以采用EM算法进行估计：</p>
<ol>
<li>初始化</li>
<li>迭代直到收敛</li>
<li>E-step：写出Ez(log p(x,z|\theta))，p(x,z|\theta)用似然函数代替即可。</li>
<li>M-step：最大化上式，求得\theta，转至第2步</li>
</ol>
<p>EM算法的作用不只这些，考虑简单的Bayesian LinReg，其中的w也可以认为是LV，服从正态，x,y是observed var.，服从正态。参数估计完全也可以采用EM算法。还有可以看出，mixture Gaussian也是一种距离学习，和k-means，kNN，LDA等一类都是共通的。</p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/05/04/ML12/" class="next">NEXT</a></div><div data-thread-key="2016/05/05/ML14/" data-title="闲谈机器学习(14)" data-url="http://yoursite.com/2016/05/05/ML14/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"seansun"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2016 <a href="http://yoursite.com">ClT</a>, unless otherwise noted.</p></div></footer><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>