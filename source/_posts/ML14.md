---
title: 闲谈机器学习(14)
date: 2016-05-05 14:46:05
tags:
---
隐变量模型(LVM)也是机器学习中一种常见模型，隐变量(LV)是observed var.的反义词。从广义角度来说，所谓学习不也就是获得这些隐变量吗？

狭义而言，LVM主要包括：

* mixture model
* factor analysis
* PCA（主成分分析）
* ICA（独立成分分析）
* LDA

mixture model的变量为连续分布，隐变量为离散分布；因子分析则都是连续分布；PCA基于特征值分解；独立成分分析则是信号处理的内容。

EM算法是估计LVM中参数的主要算法。下面主要说EM算法的思想。

设X为observed var.，Z为LV，\theta为参数。因为我们只观察到了X，使用MLE，先来看marginal likelihood：

<img src="https://upload.wikimedia.org/math/5/e/4/5e48c3658bb2893d425a52e4d31bde3b.png">

现在最大化这个函数就可以得到\theta的值了。麻烦的是Z和\theta都是未知的，这就很难求了。

从直观的角度来思考，如果隐变量z的分布和观察到x之后“猜测”z的分布应该尽可能接近。即期望：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/5.5.1.gif">

q表示z服从的分布。

构造如下式子：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/5.5.2.gif">

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/5.5.3.gif">

需要最大化这个式子：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/5.5.4.gif">

发现没有，EM算法前一部分已经出现。我们只管这部分，采用2阶段优化，就会得到一个近似解。

具体步骤如下：

1. 初始化
2. 迭代直到收敛
3. 计算log p(x,z|\theta)，这里面既有变量，也有隐变量，还有参数。由于EM算法尤其适用于指数分布族，所以log之后形式会比较简单。
4. E-step：对log p(x,z|\theta)关于z求期望，其中z用z的估计值代替，z的估计值由上一步的\theta求得。
5. M-step：最大化上式，求得\theta。转至第2步

mixture Gaussian是典型的LVM模型，observed var.约定为高斯分布，LV约定为Cat分布。其中待估参数有3大类。

可以采用EM算法进行估计：

1. 初始化
2. 迭代直到收敛
3. E-step：写出Ez(log p(x,z|\theta))，p(x,z|\theta)用似然函数代替即可。
4. M-step：最大化上式，求得\theta，转至第2步

EM算法的作用不只这些，考虑简单的Bayesian LinReg，其中的w也可以认为是LV，服从正态，x,y是observed var.，服从正态。参数估计完全也可以采用EM算法。还有可以看出，mixture Gaussian也是一种距离学习，和k-means，kNN，LDA等一类都是共通的。