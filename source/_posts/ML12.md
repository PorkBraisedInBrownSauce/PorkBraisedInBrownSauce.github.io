---
title: 闲谈机器学习(12)
date: 2016-05-04 12:46:05
tags:
---
Naive Bayes是著名的机器学习领域的十大著名算法之一，也是一种高效的生成式算法。Naive Bayes基于一个重要假设：条件独立性。我们说过，这是一个生成式算法，所以必然要对联合分布进行建模，但是由于得到一个类别的feature有很多，如果使用概率的乘法公式，计算确实复杂，并且不同feature之间的相关性我们也不清楚，那么姑且认为，同一类别的前提下，不同feature之间相互独立：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.1.png">

得到了likelihood，假设p(x|y)服从伯努利分布，先验p(y)服从分类分布。那么，


<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.2.png">

得到MLE的结果，

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.3.png">

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.4.png">

OK，那么naive Bayes也就得到了。这里需要注意，条件独立性的重要作用，不同的条件独立性假设将得到不同的概率图模型（PGM），Naive Bayes也是一种简单的PGM。

### Bayesian框架
这部分又将进入贝叶斯统计的框架。首先确定先验，还是使用共轭先验：
<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.6.png">

仿照Naive Bayes，写出predictive:

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.5.gif">

注意到，前一部分是Dir-Cat model，后一部分是Beta-Bern model。使用这两个model的posterior mean替换参数：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.29.7.png">

这也就是所谓的Bayesian Naive Bayes。