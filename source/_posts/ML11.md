---
title: 闲谈机器学习(11)
date: 2016-04-20 15:46:05
tags:
---
Gaussian Discriminant Analysis(GDA)是最基本的一类生成式模型，用于分类。假设某一类别的数据服从Gaussian，类别的出现服从Bern分布，则：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.20.1.gif">

实际的优化问题与距离相关，所以这可以看做一种kNN学习。

参数估计出来之后，posterior就可以轻易得到：

对于多分类，如果x|y的cov不相同，则称为Quadratic discriminant analysis(QDA)：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.20.2.png"height = "60">

式子很复杂，数学家喜欢看着优美点的，考虑特殊情况x|y的cov相同，此时是Linear discriminant analysis(LDA)：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.20.3.png"height = "90">

注意到，Bayes Theorem的归一化分母没有写。如果添加上，会发现：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.20.4.gif">

这是源于热力学的归一化的Boltzmann函数，也就是Boltzmann分布，其实也是多分类的sigmoid的函数。至于，为什么叫做LDA也是很明确的，因为含有线性scoring。QDA的分子分母无法化简，表现为二次scoring。

其实刚才已经言明，二分类LDA相似于LogReg，LDA也具备kNN这类惰性学习算法的特点。

再从统计学的角度来看LDA，也就是著名的Fisher's linear discriminant(FLD)，这个方法是由著名统计学家Fisher提出的，时间早于lDA，但是却有惊人的相似。

FLD不依赖于分布函数和Bayes方法，更像是一种判别式方案。线性判别式模式识别常见的任务，等价于机器学习的分类，但是不一定用学习的方法。普通的线性判别有一个很大的问题：高维和低维的不一致性。高维数据和低维数据的判别超平面是不一样的。那么如何让高维和低维统一起来呢？作为统计学家的Fisher提出了基于统计学的方法。

我们先考虑2维线性判别，它的思想很朴素：将数据投影到一条直线上，使得同类数据尽可能近，异类数据尽可能远。做一下简单的推导，定义两个类别的均值和协方差，则均值和协方差在分界线上的值分别为：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.20.5.gif">

此时，希望异类的均值距离远，同类的协方差尽可能近：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.20.6.gif">

这个优化问题等于广义Rayleigh商：

<img src="http://7xs6jl.com1.z0.glb.clouddn.com/4.20.7.gif">

多分类的FLD类似于二分类的FLD。

LDA不仅可以用于分类，还可以用于降维。注意只要学习出w，那么可以把N维数据投影到N-1维空间。

为什么说FLD和LDA殊途同归呢？考虑最上面的式子，我们说等同于kNN，也就是类内间距足够小。类间间距由y的prior决定。所以，结论是LDA=FLD，他们和kNN，LogReg相似。


