<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ClT&#39;s Blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-05-20T05:01:24.896Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ClT</name>
    <email>cl.tian@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>翻墙hosts切换小程序</title>
    <link href="http://yoursite.com/2016/05/20/simple_host/"/>
    <id>http://yoursite.com/2016/05/20/simple_host/</id>
    <published>2016-05-20T04:27:02.000Z</published>
    <updated>2016-05-20T05:01:24.896Z</updated>
    
    <content type="html">&lt;p&gt;校园网和中科院网络一般都是支持ipv6的，此时可以使用ipv6访问youtube，google，且不要流量。但有时候出了校园网又会有些不方便，写了一个小python程序，可以实现切换和hosts更新。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-
&amp;quot;&amp;quot;&amp;quot;
Created on Fri May 13 21:45:16 2016

@author: ClT
&amp;quot;&amp;quot;&amp;quot;

import Tkinter as tk
import os
import shutil
import urllib2
path = r&amp;quot;C:\Windows\System32\drivers\etc&amp;quot;
cur = os.path.split(os.path.realpath(__file__))[0]
url = r&amp;quot;http://7xs6jl.com1.z0.glb.clouddn.com/hosts&amp;quot;
class Application(tk.Frame):

    def trepair(self):
        if os.path.isfile(os.path.join(path,&amp;quot;hosts&amp;quot;)) and not os.path.isfile(os.path.join(path,&amp;quot;hosts.backup&amp;quot;)):
            os.rename(os.path.join(path,&amp;quot;hosts&amp;quot;),os.path.join(path,&amp;quot;hosts.backup&amp;quot;))
            shutil.copy(os.path.join(cur,&amp;quot;temp&amp;quot;), path)
            os.rename(os.path.join(path,&amp;quot;temp&amp;quot;),os.path.join(path,&amp;quot;hosts&amp;quot;))
    def trecover(self):
        if os.path.isfile(os.path.join(path,&amp;quot;hosts.backup&amp;quot;)):
            os.remove(os.path.join(path,&amp;quot;hosts&amp;quot;))
            os.rename(os.path.join(path,&amp;quot;hosts.backup&amp;quot;),os.path.join(path,&amp;quot;hosts&amp;quot;))

    def tupdate(self):
        f1 = urllib2.urlopen(url)
        data = f1.read()
        with open(os.path.join(cur,&amp;quot;temp&amp;quot;),&amp;apos;w&amp;apos;) as f2:
            f2.truncate()
            f2.write(data)

    def createWidgets(self):
        self.recover = tk.Button(self)
        self.recover[&amp;quot;text&amp;quot;] = &amp;quot;复原&amp;quot;
        self.recover[&amp;quot;fg&amp;quot;]   = &amp;quot;red&amp;quot;
        self.recover[&amp;quot;command&amp;quot;] =  self.trecover
        self.recover.grid(row=3)
        self.repair = tk.Button(self)
        self.repair[&amp;quot;text&amp;quot;] = &amp;quot;上网&amp;quot;
        self.repair[&amp;quot;fg&amp;quot;]   = &amp;quot;blue&amp;quot;
        self.repair[&amp;quot;command&amp;quot;] = self.trepair
        self.repair.grid(row=2)     
        self.label = tk.Label(self, text = &amp;quot;hello&amp;quot;)      
        self.update = tk.Button(self)
        self.update[&amp;quot;text&amp;quot;] = &amp;quot;更新&amp;quot;
        self.update[&amp;quot;command&amp;quot;] = self.tupdate
        self.update.grid(row=4)

    def __init__(self, master=None):
        tk.Frame.__init__(self, master)
        self.pack()
        self.createWidgets()

root = tk.Tk()
app = Application(master=root)
app.mainloop()
root.destroy()
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;p&gt;校园网和中科院网络一般都是支持ipv6的，此时可以使用ipv6访问youtube，google，且不要流量。但有时候出了校园网又会有些不方便，写了一个小python程序，可以实现切换和hosts更新。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>组合优化</title>
    <link href="http://yoursite.com/2016/05/16/%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2016/05/16/组合优化/</id>
    <published>2016-05-16T14:46:05.000Z</published>
    <updated>2016-05-20T04:48:37.700Z</updated>
    
    <content type="html">&lt;p&gt;由于数学能力有限，只能说大白话。&lt;/p&gt;
&lt;p&gt;首先说一下什么是组合优化。组合优化是优化的一个分支，与实优化相对的概念，也可以叫做离散优化。它的优化目标集是一个有限集，例如整数优化、0/1规划等。由于有限集的特点，一般可以通过遍历达到解决组合优化问题的目的，但是不论是子集，还是排列、组合等问题，如果采取遍历，往往问题规模是指数级别的大小，即所谓的指数爆炸，所以就有所谓的组合优化问题。常见的组合优化问题：最短路问题、最小生成树问题、网络流问题、匹配问题、TSP问题、SAT问题、汉密尔顿圈问题等。&lt;/p&gt;
&lt;p&gt;离散域是不可导的（因为不连续）。数值最优化方法不能带到离散域中。离散域中的问题往往是一个问题一个方法，导致组合优化与数值优化有很大区别。&lt;/p&gt;
&lt;h2 id=&quot;I-多项式级别的组合优化算法&quot;&gt;&lt;a href=&quot;#I-多项式级别的组合优化算法&quot; class=&quot;headerlink&quot; title=&quot;I.多项式级别的组合优化算法&quot;&gt;&lt;/a&gt;I.多项式级别的组合优化算法&lt;/h2&gt;&lt;p&gt;常见的多项式组合优化算法有4种：短路问题、最小生成树问题、网络流问题、匹配问题。并由之派生出一些其他问题。他们的解法大部分属于&lt;strong&gt;primal-dual+search&lt;/strong&gt;的设计思路，也有用动态规划思想的。&lt;/p&gt;
&lt;p&gt;primal-dual方法是多项式组合优化的核心方法之一，它是利用线性规划（LP）的原始对偶方法将一个大问题化解为小问题一步一步解决，并从中观察得出算法的策略。简而言之，一切优化问题都要满足可行性和最优性，primal-dual算法的目的是保持最优性，逐步消除不可行性。&lt;/p&gt;
&lt;h3 id=&quot;1-最短路问题&quot;&gt;&lt;a href=&quot;#1-最短路问题&quot; class=&quot;headerlink&quot; title=&quot;1.最短路问题&quot;&gt;&lt;/a&gt;1.最短路问题&lt;/h3&gt;&lt;p&gt;最短路问题主要有Dijkstra算法、Bellman-Ford算法、Floyd-Warshall算法等。Dijkstra算法采用primal-dual的思想，后两个算法采用动态规划。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dijkstra算法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;写出最短路问题的LP：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中A为点弧关联矩阵，由于A满足全单模条件，所以去除了f=0,1的约束。由于问题的特殊结构可以删除一行约束：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.4.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;写出dual:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;再写出DRP:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时就得到DRP，DRP给出了如何improve的方法：不断改进以使问题达到可行。一定要注意到这里的最短路径问题含有非常多的特殊结构，包括全单模、秩的特点等等。&lt;/p&gt;
&lt;p&gt;如果我们拿出一个例子会惊异的发现，从s出发，初始解为(0,0,…)’，DRP的求解过程实际是一个贪心的过程。&lt;/p&gt;
&lt;p&gt;直观的说，dual问题中的y被称为“势”，可以解释为“海拔”，dual问题提供了primal的下界，由于这是LP，所以，最优解要满足dual的解=primal的解。s-t的海拔实际是将所有城市比拟为绳子上的小球，权重解释为小球之间的绳子长度，那么将s点和t点拉起来，实际就得到了所谓的最短路径，也是最大海拔。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Bellman-Ford算法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bellman-Ford算法使用DP的技术，可以处理还有负圈的最短路径问题（Dijkstra算法为什么不可以？）。&lt;/p&gt;
&lt;p&gt;使用动态规划，要确定决策变量：这里有3个i,j,k。i表示起点，j表示终点，k表示i到j所需最多的边（确定好决策变量这一步是很困难的），那么得到递归式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.6.png&quot; width=&quot;400&quot; height=&quot;50&quot;&gt;&lt;/p&gt;
&lt;p&gt;return OPT[s,t,n-1]&lt;/p&gt;
&lt;p&gt;含义是很明确的：每一步的决策或者是上一步的决策（最多使用k-1条边就可以达成），或者有新的边加进来。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Floyd-Warshall算法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Floyd-Warshall算法可以解决任意两点之间的最短路径问题。也采用DP的策略。&lt;/p&gt;
&lt;p&gt;同样有3个决策变量：i,j,k。i表示起点，j表示终点，k表示i到j以(1..k)中的节点为中间节点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/8/d/a/8dad6c9c6f54c849e730c19a48598c84.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;若i到j最短路径不经过k，则&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/b/6/f/b6fd348302496399233dce43e708aa21.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;若经过k，则&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/e/3/a/e3aee6a3e2a2083164cdfee6f1c9ad03.png&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-网络流问题&quot;&gt;&lt;a href=&quot;#2-网络流问题&quot; class=&quot;headerlink&quot; title=&quot;2.网络流问题&quot;&gt;&lt;/a&gt;2.网络流问题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ford-Fulkerson算法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ford-Fulkerson算法是网络流的最原始算法。写出LP：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.7.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中，A为点弧关联矩阵，f为流量，dv表示中间节点流量守恒，源点、汇点流量为v,-v。&lt;/p&gt;
&lt;p&gt;这本来就是一个dual，写出DRP：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.8.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;发现一个重要问题：f&amp;lt;=0与f&amp;gt;=0，表示：&lt;/p&gt;
&lt;p&gt;饱和弧是后向弧；空弧是前向弧。&lt;/p&gt;
&lt;p&gt;写出逆否命题：&lt;/p&gt;
&lt;p&gt;前向弧是非饱和；后向弧是非空弧。&lt;/p&gt;
&lt;p&gt;improve的目标是搜索这样的路，被称为“增广路”。然后让它们由不可行变为可行。&lt;/p&gt;
&lt;p&gt;看看如何改善：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;我多次说过特殊结构，有多项式算法的问题无一例外都是具有特殊结构的问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.9.png&quot; width=&quot;300&quot; height=&quot;50&quot;&gt;&lt;/p&gt;
&lt;p&gt;有这个式子，当然可以进行算法设计了。但是，一般我们可以由这个式子设计一种特殊的图：剩余图。使得剩余图+上一步迭代的图=新图。&lt;/p&gt;
&lt;p&gt;得出了Ford-Fulkerson算法基本步骤：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;init f(e)=0 for all e
while there&amp;apos;s an s-t path in residual graph:
    arbitrarily choose an s-t path p in residual graph
    choose a bottleneck to Augment(p,f)
return f 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Edmonds-Karp算法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ford-Fulkerson算法有一个很大的缺点：当容量限制不合理的时候回出现长久跳不出循环或者无限循环的问题。&lt;/p&gt;
&lt;p&gt;如果我们对算法中的一些步骤做一下限制，能不能克服这个缺点？这个改进就是Edmonds-Karp算法，它每次选择最短的s-t路而不是任意选择。也就是加入了search的策略。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;init f(e)=0 for all e
while there&amp;apos;s an s-t path in residual graph:
    BFS choose an shortest s-t path p in residual graph
    choose a bottleneck to Augment(p,f)
return f 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;最小费用流算法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;列出最小费用流的LP：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.10.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;将min c’f变为max -c’f，然后变为DRP：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.11.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;注意约束也很特别，尤其是Af=0，结合Ford-Fulkerson算法，这里的含义是：寻找空弧非负，饱和弧非正的循环流。找到之后，improve。&lt;/p&gt;
&lt;p&gt;那也就是说，我们找到一个特殊的循环流，它的花费是负的，那么这个图就有改善的可能。这也就是所谓的Klein圈算法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hitchcock问题&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hitchcock问题指派问题的推广，也涵盖了二部图的最小费用流。它的求解算法对指派问题都很有借鉴。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.16.2.png&quot; width=&quot;250&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以将这个问题建模为最小费用流，这里探讨primal-dual的思路，得到一个应用更为广泛的算法。写出dual：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.16.3.png&quot; width=&quot;300&quot;&gt;&lt;/p&gt;
&lt;p&gt;写出DRP：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.16.5.png&quot; width=&quot;230&quot;&gt;&lt;/p&gt;
&lt;p&gt;再写出RP:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.16.4.png&quot; width=&quot;230&quot;&gt;&lt;/p&gt;
&lt;p&gt;为什么平时不写RP，这里要写出RP。可以看出RP比较繁琐一点，而且只要有dual就可以推出DRP，似乎RP没有存在的价值。但是这里的RP可以做一个小小的变换：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.16.6.png&quot; width=&quot;500&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时可以得到等价的RP’：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.16.7.png&quot; width=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;RP’实际是一个最大流问题。现在我们的解决方案似乎是这样：dual-&amp;gt;primal-&amp;gt;RP’-&amp;gt;DRP这样循环去做，其中的核心步骤是RP’负责运行最大流算法，找出需要improve的节点；DRP负责improve。&lt;/p&gt;
&lt;h3 id=&quot;3-匹配问题&quot;&gt;&lt;a href=&quot;#3-匹配问题&quot; class=&quot;headerlink&quot; title=&quot;3.匹配问题&quot;&gt;&lt;/a&gt;3.匹配问题&lt;/h3&gt;&lt;p&gt;匹配问题也是多项式组合优化问题中的一大类重要问题。基本包含4种：二部图基数匹配，二部图赋权匹配，非二部图基数匹配，非二部图赋权匹配。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;二部图基数匹配&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;匹配问题较难写出LP，故而主要借鉴其他算法的思路，结合search方法。&lt;/p&gt;
&lt;p&gt;最简单的思路是直接建模为最大流，添加源点和汇点，将容量限制设为1，采用Edmonds-Karp算法就可以求出。那有没有更一般的算法呢？这就是著名的“匈牙利算法”。&lt;/p&gt;
&lt;p&gt;匈牙利算法基于很简单的观察：如果我得到一个匹配，如何improve？&lt;/p&gt;
&lt;p&gt;此时如果存在这样一条路：从未盖点出发，依次经过以盖点，未盖点，…，直到未盖点。那么未盖点-以盖点=1，也就是说，此时将以盖点变为未盖点，未盖点变为以盖点就可以多覆盖一个点，improved！这条路也叫“增广路”，而以盖点、未盖点依次出现的路叫做交错路。&lt;/p&gt;
&lt;p&gt;以下是伪代码：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bool 寻找从k出发的对应项出的可增广路
{
    while (BFS from k to j)
      {
        if (j不在增广路上)
        {
            把j加入增广路;
            if (j是未盖点 或者 从j的对应项出发有可增广路)
            {
                修改j的对应项为k;
                则从k的对应项出有可增广路,返回true;
            }
        }
    }
    则从k的对应项出没有可增广路,返回false;
}

void Hungary()
{
    for i-&amp;gt;1 to n
    {
        if (则从i的对应项出有可增广路)
            匹配数++;
    }
    输出 匹配数;
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;二部图赋权匹配&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;二部图最大赋权匹配是二部图基数匹配的推广。w:=max{wi}-w，通过这个式子，将二部图最大赋权匹配转化为二部图最小赋权匹配。&lt;/p&gt;
&lt;p&gt;那么仍然可以直接model到网络流问题，也就是最小费用流问题。其次可以采用匈牙利算法，具体来讲是Kuhn–Munkres算法。还可以采用primal-dual的方法。&lt;/p&gt;
&lt;p&gt;二部图赋权匹配属于指派问题，可以采用Hitchcock问题的算法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.16.1.png&quot; width=&quot;150&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;非二部图匹配&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;非二部图问题与二部图问题的主要区别在于：二部图中，增广路不可能构成圈，那么匈牙利算法就不可能失效。非二部图中，增广路可能构成圈，而且这个圈肯定是奇圈（花），所以算法的改进在于怎么让花消失。&lt;/p&gt;
&lt;h3 id=&quot;4-拟阵与最小生成树问题&quot;&gt;&lt;a href=&quot;#4-拟阵与最小生成树问题&quot; class=&quot;headerlink&quot; title=&quot;4.拟阵与最小生成树问题&quot;&gt;&lt;/a&gt;4.拟阵与最小生成树问题&lt;/h3&gt;&lt;p&gt;拟阵是满足以下两条定义的集合关系：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.15.12.png&quot; width=&quot;550&quot;&gt;&lt;/p&gt;
&lt;p&gt;实际上，它是对线性代数中的向量之间的相互独立的数学抽象。想想向量之间的相互独立是不是也具有以上的性质。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.16.8.png&quot; height=&quot;300&quot;&gt;&lt;/p&gt;
&lt;p&gt;拟阵有一个最为基本的优化性质：极大独立集一定是最大独立集合——表示为拟阵的 Greedy 算法。&lt;/p&gt;
&lt;p&gt;其中最小生成树就是利用了这个性质而设计的Greedy算法。不论是Kruskal算法，还是Prim算法，他们都遵循贪婪的思想。&lt;/p&gt;
&lt;h2 id=&quot;II-复杂问题的解法&quot;&gt;&lt;a href=&quot;#II-复杂问题的解法&quot; class=&quot;headerlink&quot; title=&quot;II.复杂问题的解法&quot;&gt;&lt;/a&gt;II.复杂问题的解法&lt;/h2&gt;&lt;p&gt;复杂问题中有一部分是可以被证明是NP-Complete问题，有一部分是悬而未决的NP问题。对于复杂问题既可以精确求解，也可以求出近似解。&lt;/p&gt;
&lt;h3 id=&quot;分支限界&quot;&gt;&lt;a href=&quot;#分支限界&quot; class=&quot;headerlink&quot; title=&quot;分支限界&quot;&gt;&lt;/a&gt;分支限界&lt;/h3&gt;&lt;p&gt;分支限界是典型的穷举剪枝算法，和回溯法是一样的思路，不同在于回溯法的回溯是确定的，而B&amp;amp;B中限界在于个人的选择，限界标准的好坏直接影响算法的好坏。如果界太紧，很容易丢弃可行解；如果界太松，算法效率低。&lt;/p&gt;
&lt;h3 id=&quot;动态规划&quot;&gt;&lt;a href=&quot;#动态规划&quot; class=&quot;headerlink&quot; title=&quot;动态规划&quot;&gt;&lt;/a&gt;动态规划&lt;/h3&gt;&lt;p&gt;动态规划(dynamic programming)是求解多阶段决策优化问题的一种有效方法。它可以用于离散、连续、随机等多种类型的问题，应用领域非常广泛。设计动态规划的一种典型思路是给出需要求解的实例最优解与另一规模较小的实例最优解之间的递推关系。由于规模最小的那些实例最优解容易求得，从而可以以此为初始条件，进而利用递推关系一步一步求得最优解。&lt;/p&gt;
&lt;p&gt;一个可以使用DP求解的问题必须满足：可分解为相互关联的子问题，最优化定理，无后效性。&lt;/p&gt;
&lt;h3 id=&quot;近似算法和近似方案&quot;&gt;&lt;a href=&quot;#近似算法和近似方案&quot; class=&quot;headerlink&quot; title=&quot;近似算法和近似方案&quot;&gt;&lt;/a&gt;近似算法和近似方案&lt;/h3&gt;&lt;p&gt;对于 NP—难的组合优化问题，如果实例规模比较大，求得最优解往往需要相当长的时间。因此，如果时间上的限制比对精度上的要求更严格，可以考虑“用精度换时间”，即在多项式时间内得到一个与最优解较为接近的可行解，称为近似解。&lt;/p&gt;
&lt;p&gt;启发式算法是近似算法的一种，可以用于组合优化问题的有：模拟退火算法、遗传算法、蚁群算法等等。&lt;/p&gt;
&lt;p&gt;Greedy也是可以用于解决难问题的思路。满足贪心选择性的问题不多，但是如果有一个好的标准，Greedy也许能得到满意的结果，并且速度很快。实际上很多好的近似算法都是在Greedy算法基础上做改进，让贪心也许能少贪点得出的好算法。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;由于数学能力有限，只能说大白话。&lt;/p&gt;
&lt;p&gt;首先说一下什么是组合优化。组合优化是优化的一个分支，与实优化相对的概念，也可以叫做离散优化。它的优化目标集是一个有限集，例如整数优化、0/1规划等。由于有限集的特点，一般可以通过遍历达到解决组合优化问题的目的，但是不论是子集，
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(14)</title>
    <link href="http://yoursite.com/2016/05/05/ML14/"/>
    <id>http://yoursite.com/2016/05/05/ML14/</id>
    <published>2016-05-05T06:46:05.000Z</published>
    <updated>2016-05-06T08:41:17.491Z</updated>
    
    <content type="html">&lt;p&gt;隐变量模型(LVM)也是机器学习中一种常见模型，隐变量(LV)是observed var.的反义词。从广义角度来说，所谓学习不也就是获得这些隐变量吗？&lt;/p&gt;
&lt;p&gt;狭义而言，LVM主要包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mixture model&lt;/li&gt;
&lt;li&gt;factor analysis&lt;/li&gt;
&lt;li&gt;PCA（主成分分析）&lt;/li&gt;
&lt;li&gt;ICA（独立成分分析）&lt;/li&gt;
&lt;li&gt;LDA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;mixture model的变量为连续分布，隐变量为离散分布；因子分析则都是连续分布；PCA基于特征值分解；独立成分分析则是信号处理的内容。&lt;/p&gt;
&lt;p&gt;EM算法是估计LVM中参数的主要算法。下面主要说EM算法的思想。&lt;/p&gt;
&lt;p&gt;设X为observed var.，Z为LV，\theta为参数。因为我们只观察到了X，使用MLE，先来看marginal likelihood：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/5/e/4/5e48c3658bb2893d425a52e4d31bde3b.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;现在最大化这个函数就可以得到\theta的值了。麻烦的是Z和\theta都是未知的，这就很难求了。&lt;/p&gt;
&lt;p&gt;从直观的角度来思考，如果隐变量z的分布和观察到x之后“猜测”z的分布应该尽可能接近。即期望：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.5.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;q表示z服从的分布。&lt;/p&gt;
&lt;p&gt;构造如下式子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.5.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.5.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;需要最大化这个式子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.5.4.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;发现没有，EM算法前一部分已经出现。我们只管这部分，采用2阶段优化，就会得到一个近似解。&lt;/p&gt;
&lt;p&gt;具体步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化&lt;/li&gt;
&lt;li&gt;迭代直到收敛&lt;/li&gt;
&lt;li&gt;计算log p(x,z|\theta)，这里面既有变量，也有隐变量，还有参数。由于EM算法尤其适用于指数分布族，所以log之后形式会比较简单。&lt;/li&gt;
&lt;li&gt;E-step：对log p(x,z|\theta)关于z求期望，其中z用z的估计值代替，z的估计值由上一步的\theta求得。&lt;/li&gt;
&lt;li&gt;M-step：最大化上式，求得\theta。转至第2步&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;mixture Gaussian是典型的LVM模型，observed var.约定为高斯分布，LV约定为Cat分布。其中待估参数有3大类。&lt;/p&gt;
&lt;p&gt;可以采用EM算法进行估计：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化&lt;/li&gt;
&lt;li&gt;迭代直到收敛&lt;/li&gt;
&lt;li&gt;E-step：写出Ez(log p(x,z|\theta))，p(x,z|\theta)用似然函数代替即可。&lt;/li&gt;
&lt;li&gt;M-step：最大化上式，求得\theta，转至第2步&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;EM算法的作用不只这些，考虑简单的Bayesian LinReg，其中的w也可以认为是LV，服从正态，x,y是observed var.，服从正态。参数估计完全也可以采用EM算法。还有可以看出，mixture Gaussian也是一种距离学习，和k-means，kNN，LDA等一类都是共通的。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;隐变量模型(LVM)也是机器学习中一种常见模型，隐变量(LV)是observed var.的反义词。从广义角度来说，所谓学习不也就是获得这些隐变量吗？&lt;/p&gt;
&lt;p&gt;狭义而言，LVM主要包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mixture model&lt;/li&gt;
&lt;li&gt;fact
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(13)</title>
    <link href="http://yoursite.com/2016/05/04/ML13/"/>
    <id>http://yoursite.com/2016/05/04/ML13/</id>
    <published>2016-05-04T06:46:05.000Z</published>
    <updated>2016-05-06T11:59:28.979Z</updated>
    
    <content type="html">&lt;p&gt;Bayesian Network（也叫做Belief Network）是概率图模型的重要一种，它借助DAG刻画属性间的依赖关系。因为此处确实图很多，我主要采用文字描述。以下是一个简单的BN：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://upload.wikimedia.org/wikipedia/commons/f/fd/SimpleBayesNetNodes.svg&quot;&gt;&lt;/p&gt;
&lt;p&gt;“事物之间的联系太过于复杂，加上一些看起来不是很过分的条件，m进行modeling。”&lt;/p&gt;
&lt;p&gt;BN的假设仍然是条件独立性假设：在父节点确定的条件下，后代之间相互独立。这样联合概率分布就很容易写了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/5.4.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;BN中2变量只有1种拓扑关系，3变量主要有3种基本拓扑关系：common parent, V-structure, sequential structure。&lt;/p&gt;
&lt;p&gt;这3种图中，common parent属于基本的子代互相独立，given parent；V-structure则是parent互相独立；sequential structure是爷爷与孙子相互独立，given parent。&lt;/p&gt;
&lt;p&gt;对于复杂一点的DAG如何搞清楚条件独立性是一个问题，研究通常喜欢分而治之，我们想用已有知识去解决复杂问题，就引入了d-separation的概念。通过d-seperation将DAG-&amp;gt;moral graph就可以清楚地看出所有属性之间的条件独立性。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This definition can be made more general by defining the “d”-separation of two nodes, where d stands for directional. Let P be a trail (that is, a collection of edges which is like a path, but each of whose edges may have any direction) from node u to v. Then P is said to be d-separated by a set of nodes Z if and only if (at least) one of the following holds:&lt;/p&gt;
&lt;p&gt;P contains a chain, u ← m ← v, such that the middle node m is in Z,&lt;/p&gt;
&lt;p&gt;P contains a fork, u ← m → v, such that the middle node m is in Z, or&lt;/p&gt;
&lt;p&gt;P contains an inverted fork (or collider), u → m ← v, such that the middle node m is not in Z and no descendant of m is in Z.&lt;/p&gt;
&lt;p&gt;Thus u and v are said to be d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are called d-connected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;随机变量（随即向量）之间的相依关系很容易用图表示，对于理解概率模型非常有帮助：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/LinRegPM.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/BLinRegPM.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;由于BN的强大表示型，必然导致学习难度高，不仅要学习概率值，还要学习结构。就如同ANN一样，一般会结合不同问题设计不同结构，学习数值。一类简单的BN是隐马尔科夫模型（HMM）。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Bayesian Network（也叫做Belief Network）是概率图模型的重要一种，它借助DAG刻画属性间的依赖关系。因为此处确实图很多，我主要采用文字描述。以下是一个简单的BN：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://upload.wikimedia.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(12)</title>
    <link href="http://yoursite.com/2016/05/04/ML12/"/>
    <id>http://yoursite.com/2016/05/04/ML12/</id>
    <published>2016-05-04T04:46:05.000Z</published>
    <updated>2016-05-06T08:37:20.871Z</updated>
    
    <content type="html">&lt;p&gt;Naive Bayes是著名的机器学习领域的十大著名算法之一，也是一种高效的生成式算法。Naive Bayes基于一个重要假设：条件独立性。我们说过，这是一个生成式算法，所以必然要对联合分布进行建模，但是由于得到一个类别的feature有很多，如果使用概率的乘法公式，计算确实复杂，并且不同feature之间的相关性我们也不清楚，那么姑且认为，同一类别的前提下，不同feature之间相互独立：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.29.1.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;得到了likelihood，假设p(x|y)服从伯努利分布，先验p(y)服从分类分布。那么，&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.29.2.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;得到MLE的结果，&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.29.3.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.29.4.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;OK，那么naive Bayes也就得到了。这里需要注意，条件独立性的重要作用，不同的条件独立性假设将得到不同的概率图模型（PGM），Naive Bayes也是一种简单的PGM。&lt;/p&gt;
&lt;h3 id=&quot;Bayesian框架&quot;&gt;&lt;a href=&quot;#Bayesian框架&quot; class=&quot;headerlink&quot; title=&quot;Bayesian框架&quot;&gt;&lt;/a&gt;Bayesian框架&lt;/h3&gt;&lt;p&gt;这部分又将进入贝叶斯统计的框架。首先确定先验，还是使用共轭先验：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.29.6.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;仿照Naive Bayes，写出predictive:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.29.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;注意到，前一部分是Dir-Cat model，后一部分是Beta-Bern model。使用这两个model的posterior mean替换参数：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.29.7.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;这也就是所谓的Bayesian Naive Bayes。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Naive Bayes是著名的机器学习领域的十大著名算法之一，也是一种高效的生成式算法。Naive Bayes基于一个重要假设：条件独立性。我们说过，这是一个生成式算法，所以必然要对联合分布进行建模，但是由于得到一个类别的feature有很多，如果使用概率的乘法公式，计算确
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(11)</title>
    <link href="http://yoursite.com/2016/04/20/ML11/"/>
    <id>http://yoursite.com/2016/04/20/ML11/</id>
    <published>2016-04-20T07:46:05.000Z</published>
    <updated>2016-05-06T08:34:47.435Z</updated>
    
    <content type="html">&lt;p&gt;Gaussian Discriminant Analysis(GDA)是最基本的一类生成式模型，用于分类。假设某一类别的数据服从Gaussian，类别的出现服从Bern分布，则：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.20.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;实际的优化问题与距离相关，所以这可以看做一种kNN学习。&lt;/p&gt;
&lt;p&gt;参数估计出来之后，posterior就可以轻易得到：&lt;/p&gt;
&lt;p&gt;对于多分类，如果x|y的cov不相同，则称为Quadratic discriminant analysis(QDA)：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.20.2.png&quot; height=&quot;60&quot;&gt;&lt;/p&gt;
&lt;p&gt;式子很复杂，数学家喜欢看着优美点的，考虑特殊情况x|y的cov相同，此时是Linear discriminant analysis(LDA)：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.20.3.png&quot; height=&quot;90&quot;&gt;&lt;/p&gt;
&lt;p&gt;注意到，Bayes Theorem的归一化分母没有写。如果添加上，会发现：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.20.4.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这是源于热力学的归一化的Boltzmann函数，也就是Boltzmann分布，其实也是多分类的sigmoid的函数。至于，为什么叫做LDA也是很明确的，因为含有线性scoring。QDA的分子分母无法化简，表现为二次scoring。&lt;/p&gt;
&lt;p&gt;其实刚才已经言明，二分类LDA相似于LogReg，LDA也具备kNN这类惰性学习算法的特点。&lt;/p&gt;
&lt;p&gt;再从统计学的角度来看LDA，也就是著名的Fisher’s linear discriminant(FLD)，这个方法是由著名统计学家Fisher提出的，时间早于lDA，但是却有惊人的相似。&lt;/p&gt;
&lt;p&gt;FLD不依赖于分布函数和Bayes方法，更像是一种判别式方案。线性判别式模式识别常见的任务，等价于机器学习的分类，但是不一定用学习的方法。普通的线性判别有一个很大的问题：高维和低维的不一致性。高维数据和低维数据的判别超平面是不一样的。那么如何让高维和低维统一起来呢？作为统计学家的Fisher提出了基于统计学的方法。&lt;/p&gt;
&lt;p&gt;我们先考虑2维线性判别，它的思想很朴素：将数据投影到一条直线上，使得同类数据尽可能近，异类数据尽可能远。做一下简单的推导，定义两个类别的均值和协方差，则均值和协方差在分界线上的值分别为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.20.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时，希望异类的均值距离远，同类的协方差尽可能近：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.20.6.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个优化问题等于广义Rayleigh商：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.20.7.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;多分类的FLD类似于二分类的FLD。&lt;/p&gt;
&lt;p&gt;LDA不仅可以用于分类，还可以用于降维。注意只要学习出w，那么可以把N维数据投影到N-1维空间。&lt;/p&gt;
&lt;p&gt;为什么说FLD和LDA殊途同归呢？考虑最上面的式子，我们说等同于kNN，也就是类内间距足够小。类间间距由y的prior决定。所以，结论是LDA=FLD，他们和kNN，LogReg相似。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Gaussian Discriminant Analysis(GDA)是最基本的一类生成式模型，用于分类。假设某一类别的数据服从Gaussian，类别的出现服从Bern分布，则：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clou
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>几个优化算法小程序</title>
    <link href="http://yoursite.com/2016/04/16/coding1/"/>
    <id>http://yoursite.com/2016/04/16/coding1/</id>
    <published>2016-04-16T08:46:05.000Z</published>
    <updated>2016-04-20T10:40:01.629Z</updated>
    
    <content type="html">&lt;h1 id=&quot;几个优化算法小程序&quot;&gt;&lt;a href=&quot;#几个优化算法小程序&quot; class=&quot;headerlink&quot; title=&quot;几个优化算法小程序&quot;&gt;&lt;/a&gt;几个优化算法小程序&lt;/h1&gt;&lt;p&gt;写了几个maltab版的无约束优化计算程序：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wolfe非精确线搜索步长求解&lt;/li&gt;
&lt;li&gt;Armjio非精确线搜索步长求解&lt;/li&gt;
&lt;li&gt;GD法求解非约束优化&lt;/li&gt;
&lt;li&gt;Newton法求解非约束优化&lt;/li&gt;
&lt;li&gt;BFGS quasi-Newton法求解非约束优化&lt;/li&gt;
&lt;li&gt;CG法求解非约束优化&lt;/li&gt;
&lt;li&gt;使用差分法求解Gradient和Hessian&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于都是简单的程序，计算速度不快。另外CG法的precondioning没做，LBFGS没做，Newton法是最原始的版本,所以经常会出问题。&lt;/p&gt;
&lt;p&gt;接下来，希望把LBFGS搞出来，简单的约束优化方法也准备弄出来。python版本也准备开发。&lt;/p&gt;
&lt;p&gt;&lt;s&gt;使用范例：[x,y]=tGD(@funName, w0, varargin)&lt;/s&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;u&gt;Codes are in my github, welcome pulling request&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;s&gt;&lt;strong&gt;采用100*2的feature，对Logistic回归做了一下数值模拟&lt;/strong&gt;，统计了一下错误率和训练时间(s)：&lt;/s&gt;&lt;/p&gt;
&lt;p&gt;&lt;s&gt;GD+Armijo  12%  0.3&lt;/s&gt;&lt;br&gt;&lt;s&gt;GD+Wolfe    14%  4.7&lt;/s&gt;&lt;/p&gt;
&lt;p&gt;&lt;s&gt;Newton+Armijo NaN：原因是Hessian条件数太差&lt;/s&gt;&lt;br&gt;&lt;s&gt;Newton+Wolfe  17% 17&lt;/s&gt;&lt;/p&gt;
&lt;p&gt;&lt;s&gt;CG+Armijo 16%  0.11&lt;/s&gt;&lt;br&gt;&lt;s&gt;CG+Wolfe   9%    0.35&lt;/s&gt;&lt;/p&gt;
&lt;p&gt;&lt;s&gt;BFGS+Armijo 7%    0.04&lt;/s&gt;&lt;br&gt;&lt;s&gt;BFGS+Wolfe  2.4%  0.03&lt;/s&gt;&lt;/p&gt;
&lt;p&gt;&lt;s&gt;GD+Wolfe训练时间长且错误率高；Newton+Armijo有时可能无法训练出结果，原始Newton法非常慢；BFGS效果最好。&lt;/s&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt;(Apr. 18)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;改进原始Newton法为damped modified Newton法&lt;/li&gt;
&lt;li&gt;添加LBFGS&lt;/li&gt;
&lt;li&gt;接口改变，可以设置参数&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;eg:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;options.method=’BFGS’;&lt;br&gt;options.nmax=1000;&lt;br&gt;[x,y1,time,niter]=tmin(f,x0,options,X,y);&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;数值模拟：10000*2 feature，Logistic分类&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.18.1.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;顺便试了一下SGD，发现几分钟都运行不出来，偶尔会非常快。估计主要原因是SGD等同于直接迭代，在具有数值优化功能的软件比较慢，在C一类的语言中，估计和GD差不多，但是写着简单点。&lt;/p&gt;
&lt;p&gt;小规模精确求解BFGS最好，大规模LBFGS较好。LBFGS错误率虽然有点高，但是时间优势太大。Newton法本来应该比CG法块，但是计算Hessian花了太多的时间。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&lt;strong&gt;UPDATE(Apr.20):&lt;/strong&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;已知对LBFGS的误差耿耿于怀，研究了一下，找到了问题。LogReg的训练很容易产生underflow，进而得到NaN的结果，原来也发现，然后加了一个0.01的bias。但是估计LBFGS算法本身会抛掉一些Sample，以节省内存，把bias放大了。不论调什么参数都没办法降低err。看了一下MLPP，发现可以使用log-sum-exp trick解决，试了一下，发现LBFGS错误率大大降低。&lt;/p&gt;
&lt;p&gt;使用100000*5的样本训练：&lt;/p&gt;
&lt;p&gt;LBFGS：用时0.96s，5000次迭代，错误率12.21%&lt;/p&gt;
&lt;p&gt;BFGS：用时4.625s，12次迭代，错误率10.78%&lt;/p&gt;
&lt;p&gt;另几个算法，这个规模的数据几分钟都算不出来，降低数据规模10000*5：&lt;/p&gt;
&lt;p&gt;Newton：用时37.28s，269次迭代，错误率5.42%&lt;/p&gt;
&lt;p&gt;CG：用时92.34s，2500次迭代，错误率11.22%&lt;/p&gt;
&lt;p&gt;GD：用时104.82s，2500次迭代，错误率5.74%&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;几个优化算法小程序&quot;&gt;&lt;a href=&quot;#几个优化算法小程序&quot; class=&quot;headerlink&quot; title=&quot;几个优化算法小程序&quot;&gt;&lt;/a&gt;几个优化算法小程序&lt;/h1&gt;&lt;p&gt;写了几个maltab版的无约束优化计算程序：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wolfe非
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(10)</title>
    <link href="http://yoursite.com/2016/04/14/ML10/"/>
    <id>http://yoursite.com/2016/04/14/ML10/</id>
    <published>2016-04-14T15:46:05.000Z</published>
    <updated>2016-05-06T08:34:37.851Z</updated>
    
    <content type="html">&lt;h2 id=&quot;1-Boosting&quot;&gt;&lt;a href=&quot;#1-Boosting&quot; class=&quot;headerlink&quot; title=&quot;1.Boosting&quot;&gt;&lt;/a&gt;1.Boosting&lt;/h2&gt;&lt;p&gt;和bagging相似，这也是一种针对样本的multi-stage，不同的是，Boosting采用串行策略，适用于个体学习器之间存在强依赖关系。Boosting不采用Bootstrap sampling的方法，而是根据学习器的表现对训练样本的分布进行调整。让学习错误的base learner受到关注，让学习正确的base learner减少关注。&lt;/p&gt;
&lt;p&gt;Boosting方法中最著名的是AdaBoost(Adaptive Boosting):&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.1.png&quot; width=&quot;400&quot; height=&quot;300&quot;&gt;&lt;/p&gt;
&lt;p&gt;AdaBoost确实完成了我们的想法，“对不同的孩子给予了不同的照顾”。&lt;/p&gt;
&lt;h2 id=&quot;2-Functional-amp-Optimization-Viewpoint-Of-Boosting&quot;&gt;&lt;a href=&quot;#2-Functional-amp-Optimization-Viewpoint-Of-Boosting&quot; class=&quot;headerlink&quot; title=&quot;2.Functional &amp;amp; Optimization Viewpoint Of Boosting&quot;&gt;&lt;/a&gt;2.Functional &amp;amp; Optimization Viewpoint Of Boosting&lt;/h2&gt;&lt;p&gt;上面的语言有点直观，这里将采用泛函与优化的角度说明Boosting方法。&lt;/p&gt;
&lt;p&gt;在LM中，我们一直在学习参数，具体来说是w，也就是优化这个参数。常采用的是GD或SGD法：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.4.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;在Boosting中，我们并不知道base learner是什么，这里优化的不是参数，而是函数。在函数空间找到最优的函数：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.2.png&quot; width=&quot;150&quot; height=&quot;50&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个时候当然GD法会改一改，但是框架不变：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们进行迭代算法的目标仍然是最优化“步长”和“方向”。采用exp-loss：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.6.png&quot; height=&quot;60&quot;&gt;&lt;/p&gt;
&lt;p&gt;对于二分类（+1/-1）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.7.png&quot; height=&quot;110&quot;&gt;&lt;/p&gt;
&lt;p&gt;最优化“步长”（steppest descent):&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.9.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;接下来最优化“方向”。再认真观察上面的式子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.10.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这是一个exp-loss的multi-stage，我们把所谓的方向当做base learner的话，等价到优化w，w是一个参数，比优化函数好太多，展开一阶Taylor可以得到递推式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.11.png&quot; height=&quot;110&quot;&gt;&lt;/p&gt;
&lt;p&gt;事实上，指数部分和前面求的“步长”有关。所谓的权重，换个说法，也就是抽样的概率。对比一下，其实这里推导出来的“步长”和权重已经得到了AdaBoost。&lt;/p&gt;
&lt;p&gt;采用不同的loss-func. 可以得到不同的Boosting算法，区别在于“步长”和权重：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.12.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;3-GBDT&quot;&gt;&lt;a href=&quot;#3-GBDT&quot; class=&quot;headerlink&quot; title=&quot;3.GBDT&quot;&gt;&lt;/a&gt;3.GBDT&lt;/h2&gt;&lt;p&gt;GBDT是一种Tree算法。前面没有直接求“方向”，而是采用reduction的思想。GBDT采用一定策略求这个“方向”。当然还是采用迭代的方法求解。采用平方损失。&lt;/p&gt;
&lt;p&gt;这里最难解决的还是如何求“函数方向”。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.14.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;求解方向的目标是让f(x)和y的残差越来越小。那么，&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.13.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果已知“方向”的形式，如D.T.，这实际就是一个回归问题了，已经有很好的数值解法。求解出了“函数方向”，“步长”就容易多了：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.15.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这是一个一维回归问题。算法基本如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.14.16.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后其实就是返回了方向和步长乘积的和。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Boosting&quot;&gt;&lt;a href=&quot;#1-Boosting&quot; class=&quot;headerlink&quot; title=&quot;1.Boosting&quot;&gt;&lt;/a&gt;1.Boosting&lt;/h2&gt;&lt;p&gt;和bagging相似，这也是一种针对样本的multi-stage，不同的是，
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(9)</title>
    <link href="http://yoursite.com/2016/04/14/ML9/"/>
    <id>http://yoursite.com/2016/04/14/ML9/</id>
    <published>2016-04-14T09:46:05.000Z</published>
    <updated>2016-05-06T08:34:28.091Z</updated>
    
    <content type="html">&lt;h2 id=&quot;1-Bootstrapping&quot;&gt;&lt;a href=&quot;#1-Bootstrapping&quot; class=&quot;headerlink&quot; title=&quot;1.Bootstrapping&quot;&gt;&lt;/a&gt;1.Bootstrapping&lt;/h2&gt;&lt;p&gt;bootstrapping是统计学的一大类方法。&lt;/p&gt;
&lt;p&gt;In statistics, bootstrapping can refer to any test or metric that relies on random sampling with replacement.  This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods. Generally, it falls in the broader class of resampling methods.&lt;/p&gt;
&lt;p&gt;简单来说，bootstrap sampling给定m个样本的数据集，我们随机抽取一个样本放入采样集，再把该样本放回原始数据集，使得它仍有机会被抽到，这样经过m次随机采样，得到m个样本。&lt;/p&gt;
&lt;p&gt;由于每一次，每个样本被选取的概率都是1/m，采样m次，则某个样本始终没出现的概率为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.13.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;也就是说，会有36.8%的数据总是没有被选到，这样在validation set就可以利用这36.8%的数据。叫做“out-of-bag estimate”。实务上，只需要记录没有被选取的数据就可以。&lt;/p&gt;
&lt;h2 id=&quot;2-Bootstrap-Aggregation&quot;&gt;&lt;a href=&quot;#2-Bootstrap-Aggregation&quot; class=&quot;headerlink&quot; title=&quot;2.Bootstrap Aggregation&quot;&gt;&lt;/a&gt;2.Bootstrap Aggregation&lt;/h2&gt;&lt;p&gt;简称bagging，是机器学习中常用的并行aggregation方法。具体原理就是采用boostrap sampling，训练不同的base learners。最后组合到一起。这里的base learner要尽可能相互独立。&lt;/p&gt;
&lt;p&gt;例如将决策树作为base learner，将LinReg作为base learner，将ANN作为base learner都无不可。但是仍然要注意overfitting，如果全是精英，那么往往听不到下层的声音。。。由于bagging 关注于降低var，所以var较大的base learner可以与bagging结合，例如未剪枝的决策树。&lt;/p&gt;
&lt;h2 id=&quot;3-随机森林&quot;&gt;&lt;a href=&quot;#3-随机森林&quot; class=&quot;headerlink&quot; title=&quot;3.随机森林&quot;&gt;&lt;/a&gt;3.随机森林&lt;/h2&gt;&lt;p&gt;Random Forest是将D.T.与bagging结合起来的一种模型。但是RF在D.T.的训练过程中引入了随机属性选择机制，这样大大提高了base learner的多样性。RF往往比单纯的D.T.+bagging效果好。&lt;/p&gt;
&lt;p&gt;在RF中，岁D.T.的每个节点，先从该点属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这样得到若干棵D.T.，再将这个森林进行集成，就可以得到模型的输出。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Bootstrapping&quot;&gt;&lt;a href=&quot;#1-Bootstrapping&quot; class=&quot;headerlink&quot; title=&quot;1.Bootstrapping&quot;&gt;&lt;/a&gt;1.Bootstrapping&lt;/h2&gt;&lt;p&gt;bootstrapping是统计学的
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(8)</title>
    <link href="http://yoursite.com/2016/04/13/ML8/"/>
    <id>http://yoursite.com/2016/04/13/ML8/</id>
    <published>2016-04-13T03:46:05.000Z</published>
    <updated>2016-05-06T08:34:16.987Z</updated>
    
    <content type="html">&lt;p&gt;决策树(D.T.)属于conditional voting的multi-stage模型。D.T.的base learner是只含2层的D.T.，叫做decision stump，每一层在一定条件下进行分支。&lt;/p&gt;
&lt;p&gt;同样套入统计机器学习框架，我们关注的还是erri和Regul.两个问题。（当然还有优化方法和数值计算问题）&lt;/p&gt;
&lt;h2 id=&quot;1-Regression-Tree&quot;&gt;&lt;a href=&quot;#1-Regression-Tree&quot; class=&quot;headerlink&quot; title=&quot;1.Regression Tree&quot;&gt;&lt;/a&gt;1.Regression Tree&lt;/h2&gt;&lt;p&gt;首先看erri。对于回归，最普遍的是采用二次损失。在D.T.中，将erri解释为impurity，在树分支的过程中，impurity要越来越低，而每一步的分支要尽可能降低impurity。&lt;/p&gt;
&lt;p&gt;每一步要寻找让impurity降低最明显的feature，并且要找到这个feature让impurity降低最明显的阈值。&lt;/p&gt;
&lt;p&gt;如下例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.13.3.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;在二维空间，表示的分界面如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.13.4.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看到，RegTree实际上在用一定数量的超平面模拟分界曲面，如果分支无限下去，就可以得到曲面。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.13.5.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;2-Classification-Tree&quot;&gt;&lt;a href=&quot;#2-Classification-Tree&quot; class=&quot;headerlink&quot; title=&quot;2.Classification Tree&quot;&gt;&lt;/a&gt;2.Classification Tree&lt;/h2&gt;&lt;p&gt;对于分类，我们前面采用过0/1 loss,log-loss,cross-entropy loss,exp-loss,hindge loss,etc。这里没有得分的概念，输入空间是完全离散的，但是impurity依然适用。可以采用误分类率，信息增益，Gini index等。CART采用Gini index。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.13.6.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;直观上说，Gini反映了数据集D中随机抽取两个样本，其类别标记不一致的概率，也就是impurity。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.13.7.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;3-pruning&quot;&gt;&lt;a href=&quot;#3-pruning&quot; class=&quot;headerlink&quot; title=&quot;3.pruning&quot;&gt;&lt;/a&gt;3.pruning&lt;/h2&gt;&lt;p&gt;不论是RegTree还是ClsTree，共有的缺点是variance较大，对外部反应过于敏感，泛化能力不足。容易overfitting。这时肯定要采用Regul.，对D.T来说，就是pruning。&lt;/p&gt;
&lt;p&gt;prepruning是在D.T.的生长过程中，对每个节点在划分之前，先于val. set上的数据对比，看正确率大小，如果在val. set上正确率提升，就划分，否则就禁止划分。postpruning是在训练结束后回溯地进行pruning。prepruning不仅能防止overfitting，还可以减少训练时间。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.13.8.png&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;决策树(D.T.)属于conditional voting的multi-stage模型。D.T.的base learner是只含2层的D.T.，叫做decision stump，每一层在一定条件下进行分支。&lt;/p&gt;
&lt;p&gt;同样套入统计机器学习框架，我们关注的还是erri和R
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(7)</title>
    <link href="http://yoursite.com/2016/04/12/ML7/"/>
    <id>http://yoursite.com/2016/04/12/ML7/</id>
    <published>2016-04-12T09:46:05.000Z</published>
    <updated>2016-05-06T08:34:06.875Z</updated>
    
    <content type="html">&lt;h2 id=&quot;1-training-set-validation-set-test-set&quot;&gt;&lt;a href=&quot;#1-training-set-validation-set-test-set&quot; class=&quot;headerlink&quot; title=&quot;1.training set, validation set, test set&quot;&gt;&lt;/a&gt;1.training set, validation set, test set&lt;/h2&gt;&lt;p&gt;training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;training set：学习模型的参数&lt;/li&gt;
&lt;li&gt;validation set：模型选择和调参&lt;/li&gt;
&lt;li&gt;test set：最终的模型评估 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;固定的划分数据集的方法叫做handout，这种方法的缺点是容易导致训练数据减少，一种可行的方法是cross-validation，对于k-fold CV，将数据分为k份，数据在k-1份上做训练，在剩下来的数据上做validation。这样可以在数据集上做k次训练和测试，最终按照一定方法取值（例如均值）。由于划分数据集的方法是随机的，所以可以进行p次数据集划分，叫做p次k-fold CV。&lt;/p&gt;
&lt;h2 id=&quot;2-multi-stage&quot;&gt;&lt;a href=&quot;#2-multi-stage&quot; class=&quot;headerlink&quot; title=&quot;2.multi-stage&quot;&gt;&lt;/a&gt;2.multi-stage&lt;/h2&gt;&lt;p&gt;multi-stage是将特征进行多次映射，最终映射到输出空间。广义上讲，大部分学习器都是multi-stage的，这里的multi-stage特指将base learner aggregate to good(or better) learner。这类模型的最大优点在于降低variance，提升泛化性能，但是erri却不一定降低。所以抗overfitting的性能很好。&lt;/p&gt;
&lt;p&gt;那么问题是base learner怎样获得，base learner到better learner的映射关系如何建立。&lt;/p&gt;
&lt;p&gt;事实上，base learner的要求只有一条”good enough but different”，精确性和多样性本来就是有矛盾的。但我们想达到一种平衡。但是good enough即可，如果太好，反而可能造成overfitting（全是精英，商量的结果有可能脱离实际。。。）&lt;/p&gt;
&lt;p&gt;其次，base learner到better learner的映射关系如何建立。主要有5种：selection，uniform voting，linear voting，any voting，conditional voting。selection是uniform voting的退化情形，uniform voting是linear voting的退化情形，linear voting是any voting的退化情形，conditional voting是决策树采用的方式。&lt;/p&gt;
&lt;p&gt;方式上，对样本的方法有bootstapping和boosting，对model的方法有blending。&lt;/p&gt;
&lt;h2 id=&quot;3-Blending&quot;&gt;&lt;a href=&quot;#3-Blending&quot; class=&quot;headerlink&quot; title=&quot;3.Blending&quot;&gt;&lt;/a&gt;3.Blending&lt;/h2&gt;&lt;p&gt;从model角度，实现multi-stage的方法。具体而言，至少2-stages：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;get base learners&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;blend them to a better learner&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果采用selction，那么就属于模型选择问题。如果采用uniform voting，就等同于uninformative prior。linear voting或者叫weighted voting，如果有很好的先验知识，优于uniform voting，但是multi-stage多多少少都有blackbox的性质，精确的先验确实是比较难的。any voting非常强大，但是又有overfitting之虞。&lt;/p&gt;
&lt;p&gt;如果不采用prior，而是采用data-driven的方法，叫做stacking。简单来说，在traning set上得到base learner，把它当做一个特征转换，然后在把base learner的输出当做次级的输入，在validation set上继续训练。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-training-set-validation-set-test-set&quot;&gt;&lt;a href=&quot;#1-training-set-validation-set-test-set&quot; class=&quot;headerlink&quot; title=&quot;1.training set, 
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(6)</title>
    <link href="http://yoursite.com/2016/04/12/ML6/"/>
    <id>http://yoursite.com/2016/04/12/ML6/</id>
    <published>2016-04-12T01:46:05.000Z</published>
    <updated>2016-05-06T08:33:17.135Z</updated>
    
    <content type="html">&lt;h2 id=&quot;1-Bayesian视角下的kernel-method&quot;&gt;&lt;a href=&quot;#1-Bayesian视角下的kernel-method&quot; class=&quot;headerlink&quot; title=&quot;1.Bayesian视角下的kernel method&quot;&gt;&lt;/a&gt;1.Bayesian视角下的kernel method&lt;/h2&gt;&lt;p&gt;由于kernel天然具有协方差的性质，所以K=COV。我们将函数某点的值Xi当做随机变量，由于每一点都具有随机性，所以我们可以定义一个随机过程。这个随机过程的均值是容易得到的，协方差为K。根据最大熵原理，我们定义一个高斯随机过程（GP）。&lt;br&gt;即，&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/0/6/9/06910726f457f8e2def752f7612a810f.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;根据l2-Regul. LM的表示定理，我们考察一个高斯分布：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/e/7/0/e704a6f532e8205441c1f319942674f6.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/a/f/b/afb63bc04d1de00487843a9971042e96.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;假定w的先验分布为高斯：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/0/2/a/02ae62fe5b79030172cf227bedff6712.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;likelihood也为高斯：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/4/c/7/4c79a4eb076bf27ccf7be856b3c7c991.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;那么f的后验为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/c/f/3/cf375eacfe4aba90897016ae03759b3a.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;可见，w的先验，同时也是l2-Regul. 变为了GP的噪声，由于表示定理的作用，kernel变为了后验的随机变量。&lt;/p&gt;
&lt;h2 id=&quot;2-Bayesian-LinReg&quot;&gt;&lt;a href=&quot;#2-Bayesian-LinReg&quot; class=&quot;headerlink&quot; title=&quot;2.Bayesian LinReg&quot;&gt;&lt;/a&gt;2.Bayesian LinReg&lt;/h2&gt;&lt;p&gt;思考OLS和Ridge Reg的概率解释，在贝叶斯语义下，计算MAP的时候，已经得到了posterior，如何估计参数呢？MAP采用点估计，也可以采用Bayesian decision theory。但是与Ridge Reg不同的是罚因子不是由人工决定的，而是数据决定的。下面讨论Bayesian posterior predictive(简称predictive)： &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.24.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时y的预测和并不需要估计w，而是取决于超参数a,b。对a,b采用unimformative prior,就可以得到结果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.24.6.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Baysian模型的一大好处是：自动抗overfitting，因为计算predictive的时候已经自动对参数进行了average。但是，明显可以看到，缺点是太难推导，运气好的是这里起码可以积出来一个close form解，很多情况下，根本积不出来，这个时候就有所谓的变分推断，MCMC等高级技术。&lt;/p&gt;
&lt;h2 id=&quot;3-Bayesian-LogReg&quot;&gt;&lt;a href=&quot;#3-Bayesian-LogReg&quot; class=&quot;headerlink&quot; title=&quot;3.Bayesian LogReg&quot;&gt;&lt;/a&gt;3.Bayesian LogReg&lt;/h2&gt;&lt;p&gt;y~Bern(p)，我们找不到一个优良的先验分布，但又不想使用uninformative prior。这个时候使用appropriation来做。由于Gaussian有良好的congjugate prior，Bern和Gaussian都属于指数分布族，我们考虑使用Gaussian去近似Bern分布。（Laplace appropriation）&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.3.gif&quot;&gt;&lt;br&gt;具体不再深入。（可见PRML和MLPP）&lt;/p&gt;
&lt;h2 id=&quot;4-Bayesian-Occam’s-Razor-BIC-VC-dim&quot;&gt;&lt;a href=&quot;#4-Bayesian-Occam’s-Razor-BIC-VC-dim&quot; class=&quot;headerlink&quot; title=&quot;4.Bayesian Occam’s Razor, BIC, VC dim&quot;&gt;&lt;/a&gt;4.Bayesian Occam’s Razor, BIC, VC dim&lt;/h2&gt;&lt;p&gt;所谓的贝叶斯奥卡姆剃刀（Bayesian Occam’s Razor）,因为这个剃刀工作在贝叶斯公式的似然（P(D | h) ）上，而不是模型本身（ P(h) ）的先验概率上。关于贝叶斯奥卡姆剃刀我们再来看一个前面说到的曲线拟合的例子：如果平面上有 N 个点，近似构成一条直线，但绝不精确地位于一条直线上。这时我们既可以用直线来拟合（模型1），也可以用二阶多项式（模型2）拟合，也可以用三阶多项式（模型3），.. ，特别地，用 N-1 阶多项式便能够保证肯定能完美通过 N 个数据点。那么，这些可能的模型之中到底哪个是最靠谱的呢？前面提到，一个衡量的依据是奥卡姆剃刀：越是高阶的多项式越是繁复和不常见。然而，我们其实并不需要依赖于这个先验的奥卡姆剃刀，因为有人可能会争辩说：你怎么就能说越高阶的多项式越不常见呢？我偏偏觉得所有阶多项式都是等可能的。好吧，既然如此那我们不妨就扔掉 P(h) 项，看看 P(D | h) 能告诉我们什么。我们注意到越是高阶的多项式，它的轨迹弯曲程度越是大，到了八九阶简直就是直上直下，于是我们不仅要问：一个比如说八阶多项式在平面上随机生成的一堆 N 个点偏偏恰好近似构成一条直线的概率（即 P(D | h) ）有多大？太小太小了。反之，如果背后的模型是一条直线，那么根据该模型生成一堆近似构成直线的点的概率就大得多了。这就是贝叶斯奥卡姆剃刀。(  摘自&lt;a href=&quot;http://goo.gl/CkCyUs&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://goo.gl/CkCyUs&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;而贝叶斯方法通常使用的是不同model下的posterior：p(m|D,w)，我们的要求是找到一个model使得这个posterior最大。另一种方法是看marginal likelihood（p(D|m))，也就是积分后的likelihood，这等效于对参数averaging的结果，也就是将各种各种likelihood进行了voting，这样的likelihood自动避免了overfitting。&lt;/p&gt;
&lt;p&gt;使用Laplace approx.去重写p(D|m)，得到了log(p(D|m))的asymptotic结果，就是BIC：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/c/d/c/cdc9466caa55f66578c9660880b2f4db.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;前面一部分可以认为是erri，后面一部分是负的泛化能力。BIC越小，模型的erro越小。著名的AIC也说明了这个道理：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/7/5/a/75a00ba4d67592a77bb87db1c723ddfe.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;VC dim也说明了这个问题：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.12.9.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;平衡模型两类能力是机器学习的重要话题。最难平衡的情形就是overfitting，怎么抗overfitting就体现了model designer关于模型的先验与智慧了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.13.1.png&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Bayesian视角下的kernel-method&quot;&gt;&lt;a href=&quot;#1-Bayesian视角下的kernel-method&quot; class=&quot;headerlink&quot; title=&quot;1.Bayesian视角下的kernel method&quot;&gt;&lt;/a&gt;1.Bay
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(5)</title>
    <link href="http://yoursite.com/2016/04/11/ML5/"/>
    <id>http://yoursite.com/2016/04/11/ML5/</id>
    <published>2016-04-11T09:46:05.000Z</published>
    <updated>2016-05-06T08:38:59.883Z</updated>
    
    <content type="html">&lt;h2 id=&quot;1-Transform-OR-Basis&quot;&gt;&lt;a href=&quot;#1-Transform-OR-Basis&quot; class=&quot;headerlink&quot; title=&quot;1.Transform OR Basis&quot;&gt;&lt;/a&gt;1.Transform OR Basis&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.24.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;对x在一个basis下施以变换，使得x首先完成一个特征转换。LM是指w的线性，而不是特征特征一定是线性。&lt;/p&gt;
&lt;h2 id=&quot;2-kernel-trick&quot;&gt;&lt;a href=&quot;#2-kernel-trick&quot; class=&quot;headerlink&quot; title=&quot;2.kernel trick&quot;&gt;&lt;/a&gt;2.kernel trick&lt;/h2&gt;&lt;p&gt;我采用MLPP的说法：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Rather than defining our feature vector in terms of kernels, φ(x) = [κ(x,x 1),…,κ(x,xn)], we can instead work with the original feature vectors x, but modify the algorithm so that it replaces all inner products of the form  &amp;lt;x,x’&amp;gt; with a call to the kernel function, κ(x,x’).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;凡是在内积空间的变换总是可以有kernel与之对应。而且如前所述，kernel的一大特点是可以embedding infinite features。Representer Theorem只是kernel trick一个子集。&lt;/p&gt;
&lt;h2 id=&quot;3-kernel-kNN-and-kernel-k-means&quot;&gt;&lt;a href=&quot;#3-kernel-kNN-and-kernel-k-means&quot; class=&quot;headerlink&quot; title=&quot;3.kernel kNN and kernel k-means&quot;&gt;&lt;/a&gt;3.kernel kNN and kernel k-means&lt;/h2&gt;&lt;p&gt;kNN天然可以kernelized，因为采用l2-norm的kNN天然具备内积这一个优化条件。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.24.4.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;l2-norm的k-means也是这样。两点之间的距离可以化为kernel。&lt;/p&gt;
&lt;h2 id=&quot;4-kernel-machine&quot;&gt;&lt;a href=&quot;#4-kernel-machine&quot; class=&quot;headerlink&quot; title=&quot;4.kernel machine&quot;&gt;&lt;/a&gt;4.kernel machine&lt;/h2&gt;&lt;p&gt;GLM中feature表示为：&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.12.1.gif&quot;&gt;叫做kernel machine。&lt;/p&gt;
&lt;p&gt;如果为RBF kernel，则得到一个RBF NNet。误差采用二次误差，激活函数采用RBF函数。&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.12.2.png&quot;&gt;&lt;br&gt;Architecture of a radial basis function network. An input vector x is used as input to all radial basis functions, each with different parameters. The output of the network is a linear combination of the outputs from radial basis functions.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.12.3.png&quot;&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.12.4.png&quot;&gt;&lt;/center&gt;

&lt;p&gt;注意，这和RBF-kernelized SVM很相似，除了采用的loss func.不同，实际上，ANN可以包含一切supervised model，ANN每一层都在做feature transformation~T(x)，BP算法在argmin erri。LM只是层数较少的（2~3层）ANN而已。&lt;/p&gt;
&lt;p&gt;再注意到，如果采用uniform vote，那么这个kernel machine等同于欧氏距离的kNN算法。因为内积空间的内积度量实际也在距离度量，RBF kernel实际在度量特征的欧氏距离。&lt;/p&gt;
&lt;p&gt;如何确定ci？可以使用clustering，例如k-means。&lt;/p&gt;
&lt;p&gt;如果我不想使用unsupervised算法呢？最直接的方法是：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.12.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;但是这样会导致模型的泛化能力很成问题，计算复杂度也有点高，我们希望w尽可能稀疏一点，这样就引入了sparse vector machine，SVM就是一种sparse vector machine。&lt;/p&gt;
&lt;h2 id=&quot;5-kernel-PCA&quot;&gt;&lt;a href=&quot;#5-kernel-PCA&quot; class=&quot;headerlink&quot; title=&quot;5.kernel PCA&quot;&gt;&lt;/a&gt;5.kernel PCA&lt;/h2&gt;&lt;p&gt;PCA方法实际是在操作Gram矩阵，而Gram矩阵与kernel有天然的联系。所以kernel PCA是最天然不过的了。对于非线性相关的数据，通过kernel将其转化为线性相关问题，这样PCA就可以奏效。&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.12.6.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;则kernel为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.12.7.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个kernel可以组成kernel矩阵，也是cov矩阵，再做一点变换：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.12.8.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;对变换后的kernel矩阵做标准的PCA操作就可以得到kernel PCA&lt;/p&gt;
&lt;h2 id=&quot;6-非参统计的核方法（待）&quot;&gt;&lt;a href=&quot;#6-非参统计的核方法（待）&quot; class=&quot;headerlink&quot; title=&quot;6.非参统计的核方法（待）&quot;&gt;&lt;/a&gt;6.非参统计的核方法（待）&lt;/h2&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Transform-OR-Basis&quot;&gt;&lt;a href=&quot;#1-Transform-OR-Basis&quot; class=&quot;headerlink&quot; title=&quot;1.Transform OR Basis&quot;&gt;&lt;/a&gt;1.Transform OR Basis&lt;/h2&gt;&lt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>信息论对社会信息化的作用（转）</title>
    <link href="http://yoursite.com/2016/04/10/info-theory/"/>
    <id>http://yoursite.com/2016/04/10/info-theory/</id>
    <published>2016-04-10T14:46:05.000Z</published>
    <updated>2016-04-13T11:06:01.190Z</updated>
    
    <content type="html">&lt;h1 id=&quot;信息论对社会信息化的作用——纪念Shannon百年诞辰&quot;&gt;&lt;a href=&quot;#信息论对社会信息化的作用——纪念Shannon百年诞辰&quot; class=&quot;headerlink&quot; title=&quot;信息论对社会信息化的作用——纪念Shannon百年诞辰&quot;&gt;&lt;/a&gt;信息论对社会信息化的作用——纪念Shannon百年诞辰&lt;/h1&gt;&lt;p&gt;此文为&lt;strong&gt;王育民&lt;/strong&gt;先生所写，虽然着重于网络安全的讨论，但仍不失为一篇很有水平的科普文章。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://news.xidian.edu.cn/view-52201.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/shannon.jpg&quot; width=&quot;300&quot; height=&quot;365&quot;&gt;&lt;/p&gt;
&lt;p&gt;神人香农，21岁获得MIT硕士学位，他的硕士毕业论文奠定了数字逻辑电路设计的理论基础，11年后发表的《通信的数学理论》建立了信息论和编码理论，1年后的《保密系统的通信理论》建立了现代密码学和信息安全理论，没有这三项成果可能就没有所谓的数字时代吧。不仅如此，作为数学家的香农搞出的理论奠定了现代生活的理论基础，他也是一个工程师：能做硬件，会写程序。&lt;/p&gt;
&lt;p&gt;一个人，用自己一辈子推进了整个时代的进步，他所做的一切可能只是追寻他心中那份真正的快乐而已。纪念他最好的方式也许不是纪念，而是问问自己真正的快乐是什么。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://pan.baidu.com/s/1qYGcSnM&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;附：：Shannon - A Mathematical Theory of Communication原文&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;信息论对社会信息化的作用——纪念Shannon百年诞辰&quot;&gt;&lt;a href=&quot;#信息论对社会信息化的作用——纪念Shannon百年诞辰&quot; class=&quot;headerlink&quot; title=&quot;信息论对社会信息化的作用——纪念Shannon百年诞辰&quot;&gt;&lt;/a&gt;信息论对社
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(4)</title>
    <link href="http://yoursite.com/2016/04/09/ML4/"/>
    <id>http://yoursite.com/2016/04/09/ML4/</id>
    <published>2016-04-09T09:46:05.000Z</published>
    <updated>2016-05-06T08:33:49.683Z</updated>
    
    <content type="html">&lt;h2 id=&quot;1-kernel-Ridge-Regression&quot;&gt;&lt;a href=&quot;#1-kernel-Ridge-Regression&quot; class=&quot;headerlink&quot; title=&quot;1.kernel Ridge Regression&quot;&gt;&lt;/a&gt;1.kernel Ridge Regression&lt;/h2&gt;&lt;p&gt;将表示定理用于Ridge Regression，可以将线性Ridge Regression推广到非线性。再叙述一次表示定理：&lt;/p&gt;
&lt;p&gt;简单来说，一个优化问题表示为：argmin err(out)=l[(x1,y1,f(x1),…,(xn,yn,f(xn)]+g(||f||) &lt;/p&gt;
&lt;p&gt;此时，l(.)非负，g(.)为单调增函数，那么存在满足Mercer定理的核函数，使得解总可以写作：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;对于l2-norm regularization LM：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.1.gif&quot;&gt;&lt;br&gt;我们将看到，这个定理的强大作用。它使LM变成了非线性模型，而不需要对LM的框架更改许多。&lt;/p&gt;
&lt;p&gt;Ridge Reg可以表示为:argmin 平方损失+l2-norm regularization。完全符合表示定理。则：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;使用矩阵表示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;解得&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.4.gif&quot;&gt;&lt;br&gt;但是这个矩阵比较dense，数值解会浪费时间，且解不稳定。所以实务中用的不多。&lt;/p&gt;
&lt;h2 id=&quot;2-kernel-l2-norm-regularized-LogReg&quot;&gt;&lt;a href=&quot;#2-kernel-l2-norm-regularized-LogReg&quot; class=&quot;headerlink&quot; title=&quot;2.kernel l2-norm regularized LogReg&quot;&gt;&lt;/a&gt;2.kernel l2-norm regularized LogReg&lt;/h2&gt;&lt;p&gt;将表示定理用于l2-norm regularized LogReg，可以将线性Ridge Regression推广到非线性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个问题不像SVM具有稀疏性，实务中用的也不多。&lt;/p&gt;
&lt;h2 id=&quot;3-Probabilistic-SVM&quot;&gt;&lt;a href=&quot;#3-Probabilistic-SVM&quot; class=&quot;headerlink&quot; title=&quot;3.Probabilistic SVM&quot;&gt;&lt;/a&gt;3.Probabilistic SVM&lt;/h2&gt;&lt;p&gt;继续研究LogReg和SVM。&lt;/p&gt;
&lt;p&gt;soft-margin SVM使用hinge loss，LogReg使用log loss。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.6.jpg&quot; width=&quot;500&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看到log-loss和hinge-loss很相近，实际上SVM约等于l2-norm regularized LogReg。通常情况下，他们的性能也相当。LogReg主要优势在于可以直接解释为概率。SVM的主要优势在于SVM的解具有稀疏性，因而需要的样本也更少，开销小。&lt;/p&gt;
&lt;p&gt;我们可以将SVM和LogReg结合起来。先使用SVM得到一个线性模型。然后将转换后的数据带入LogReg，SVM中可以使用kernel，这个模型同样可以实现非线性分类。这就是所谓的2-stage model。叫做probabilistic SVM。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.7.gif&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;4-SVR&quot;&gt;&lt;a href=&quot;#4-SVR&quot; class=&quot;headerlink&quot; title=&quot;4.SVR&quot;&gt;&lt;/a&gt;4.SVR&lt;/h2&gt;&lt;p&gt;将SVM的思想推广到回归就得到了SVR。让回归曲线也尽可能“胖”一点。仿照soft-margin SVM：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.9.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;按照SVM的思路，可以导出SVR，同样可以kernel化。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-kernel-Ridge-Regression&quot;&gt;&lt;a href=&quot;#1-kernel-Ridge-Regression&quot; class=&quot;headerlink&quot; title=&quot;1.kernel Ridge Regression&quot;&gt;&lt;/a&gt;1.kernel Ri
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(3)</title>
    <link href="http://yoursite.com/2016/04/09/ML3/"/>
    <id>http://yoursite.com/2016/04/09/ML3/</id>
    <published>2016-04-09T09:45:15.000Z</published>
    <updated>2016-05-06T08:32:48.503Z</updated>
    
    <content type="html">&lt;h2 id=&quot;1-hard-margin-SVM&quot;&gt;&lt;a href=&quot;#1-hard-margin-SVM&quot; class=&quot;headerlink&quot; title=&quot;1.hard-margin SVM&quot;&gt;&lt;/a&gt;1.hard-margin SVM&lt;/h2&gt;&lt;p&gt;支持向量模型是第二大类线性模型，它着眼于直接增强模型的泛化能力。即让分界面尽可能的“胖”。SVM用于分类。&lt;/p&gt;
&lt;p&gt;如果分类器能正确分类：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;一个点在n维空间到分类超平面的距离：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;取分子为边界值：（即hard-margin）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时，我们得到一个有约束的优化问题：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.4.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个问题不好优化，把它变成一个好优化的问题（等价变换）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这是一个二次优化问题。（凸问题是最愿意见到的一个问题）对比具有hard-margin和没有margin的情形：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.6.png&quot;&gt;   &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.7.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;2-soft-margin-SVM&quot;&gt;&lt;a href=&quot;#2-soft-margin-SVM&quot; class=&quot;headerlink&quot; title=&quot;2.soft-margin SVM&quot;&gt;&lt;/a&gt;2.soft-margin SVM&lt;/h2&gt;&lt;p&gt;在前面的讨论中，我们一直假定训练样本是线性可分的，然而，现实中很多数据不是线性可分的，这个时候要进一步提升模型的泛化能力。使它也能容忍一部分线性不可分的training data。&lt;/p&gt;
&lt;p&gt;采用增加正则化项的方法：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.8.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;后面那部分就是正则化项（罚函数），也就是优化问题会同时考虑margin最大和误分类点尽可能少。但0/1损失数学性质不好，我们改为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.9.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中max(0,1-z)叫做hinge损失函数。如果换一个看法：err(out)=hinge(s)+l2-norm Regularization。又回到了经典的LM那里。&lt;/p&gt;
&lt;p&gt;换第三个看法：引入松弛变量，将式子再改写一下&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.10.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;注意到和hard-margin SVM 相比，soft-margin SVM多了松弛变量，正是一个松弛变量让margin可以自动调整。&lt;/p&gt;
&lt;h2 id=&quot;3-SVM的Dual问题&quot;&gt;&lt;a href=&quot;#3-SVM的Dual问题&quot; class=&quot;headerlink&quot; title=&quot;3.SVM的Dual问题&quot;&gt;&lt;/a&gt;3.SVM的Dual问题&lt;/h2&gt;&lt;p&gt;基于优化的primal-dual理论，我们可以得到hard-margin SVM和soft-margin SVM的dual问题。&lt;/p&gt;
&lt;p&gt;dual hard-margin SVM:&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.11.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;dual soft-margin SVM:&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.12.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;由对偶松弛条件，dual问题有较为稀疏的解，这是我们愿意看到的。&lt;/p&gt;
&lt;h2 id=&quot;4-Representer-Theorem&quot;&gt;&lt;a href=&quot;#4-Representer-Theorem&quot; class=&quot;headerlink&quot; title=&quot;4.Representer Theorem&quot;&gt;&lt;/a&gt;4.Representer Theorem&lt;/h2&gt;&lt;p&gt;kernel在数学上是一个很宽泛的概念，这里的kernel特指能够隐形的进行内积运算的函数：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.1.gif&quot;&gt;&lt;br&gt;试想，如果没有kernel，进行上述运算，运算复杂度为平方量级，如果引入kernel，会变成线性量级。其次，kernel可以很好的表示无限维的feature，以similarity为度量，将无限维的feature隐式的包含在kernel里。（embedding infinite features in kernel）&lt;/p&gt;
&lt;p&gt;观察dual hard-margin SVM和dual soft-margin SVM，里面含有内积运算。但是他们仍然只能进行线性分类，如果我们有一种映射，将低维不可分映射到足够高维，可能就会成为高维的线性可分问题。&lt;/p&gt;
&lt;p&gt;此时dual soft-margin SVM:&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;使用kernel:&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这样线性分类的SVM自然地扩展到了非线性。&lt;/p&gt;
&lt;p&gt;再深入讨论。只有核矩阵是半正定（半正定Gram矩阵）的kernel才是可用的kernel(Mercer定理)。事实上，对于一个半正定的核矩阵，总能找到一个与之对应的映射。任何一个kernel都隐式定义了一个再生希尔伯特核空间(RKHS)。（内积可以再生成原始函数的Hilbert Space）&lt;/p&gt;
&lt;p&gt;数学推导得到了重要的&lt;strong&gt;表示定理&lt;/strong&gt;：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.4.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;简单来说，一个优化问题表示为：argmin err(out)=l[(x1,y1,f(x1),…,(xn,yn,f(xn)]+g(||f||) &lt;/p&gt;
&lt;p&gt;此时，l(.)非负，g(.)为单调增函数，那么存在满足Mercer定理的核函数，使得解总可以写作：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;对于l2-norm Regul. LM：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.1.gif&quot;&gt;&lt;br&gt;我们将看到，这个定理的强大作用。它使LM变成了非线性模型，而不需要对LM的框架更改许多。&lt;/p&gt;
&lt;p&gt;将SVM的dual很容易推广到非线性情形。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-hard-margin-SVM&quot;&gt;&lt;a href=&quot;#1-hard-margin-SVM&quot; class=&quot;headerlink&quot; title=&quot;1.hard-margin SVM&quot;&gt;&lt;/a&gt;1.hard-margin SVM&lt;/h2&gt;&lt;p&gt;支持向量模型是第二大
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(2)</title>
    <link href="http://yoursite.com/2016/04/02/ML2/"/>
    <id>http://yoursite.com/2016/04/02/ML2/</id>
    <published>2016-04-02T15:04:05.000Z</published>
    <updated>2016-05-20T04:26:15.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;1-简单的线性分类模型&quot;&gt;&lt;a href=&quot;#1-简单的线性分类模型&quot; class=&quot;headerlink&quot; title=&quot;1.简单的线性分类模型&quot;&gt;&lt;/a&gt;1.简单的线性分类模型&lt;/h2&gt;&lt;p&gt;前面介绍的线性模型(LM)适应于回归。一般的线性模型可以用于分类吗？可以有以下朴素模型：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.1.gif&quot;&gt;&lt;br&gt;但是这个优化问题不可导，类似于组合优化，是一个NP-hard问题。&lt;/p&gt;
&lt;p&gt;放宽一下，不要求那么精确，我们只要看y(w’x)是否同号即可：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.2.gif&quot;&gt;&lt;br&gt;这被称为：感知机模型(perceptron)，是最简单的一个线性分类模型，也是最简单的神经网络模型(ANN)。这个问题容易优化，使用SGD（随机梯度下降法）很容易求解。但是这个模型能力非常有限，具体将在ANN部分说明。&lt;/p&gt;
&lt;h2 id=&quot;2-广义线性模型-GLM&quot;&gt;&lt;a href=&quot;#2-广义线性模型-GLM&quot; class=&quot;headerlink&quot; title=&quot;2.广义线性模型(GLM)&quot;&gt;&lt;/a&gt;2.广义线性模型(GLM)&lt;/h2&gt;&lt;p&gt;GLM是LM的推广，在统计学上有很严密的理论支撑。机器学习领域最明显的应用是：logistic回归。&lt;br&gt;这里通过3个角度简单说明LogReg。&lt;/p&gt;
&lt;p&gt;所谓GLM，是指它扩展了OLS的误差正态性假设，假设y服从指数分布族的一个分布（正态分布是指数分布族的一支）。如果y~Bern(p)(y服从伯努利分布），那么：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.3.gif&quot;&gt;&lt;br&gt;当p&amp;lt;=0.5，则y=-1;当p&amp;gt;0.5，则y=+1.&lt;br&gt;这个模型具有很强的扩展性，如果y~Cat(p1,p2,…)时，可以得到用于多分类的LogReg。（这里的具体推导不书写）&lt;/p&gt;
&lt;p&gt;第二个角度更容易理解。我们定义归一化sigmoid函数(softmax)：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.4.gif&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.5.png&quot; width=&quot;500&quot; height=&quot;200&quot;&gt;&lt;br&gt;sigmoid(w’x)相当于将值域压缩到[0,1]。&lt;/p&gt;
&lt;p&gt;并且，sigmoid函数二阶可导，g’(z)=g(z)(1-g(z))，这对优化而言是好事。对LogReg的优化采用MLE+GD/SGD/Newton/quasi-Newton等数值优化方法皆可。使用GD或者Newton法都可以迭代求解。&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.1.gif&quot;&gt;&lt;br&gt;第三个角度仍然从形式化的LM出发。&lt;/p&gt;
&lt;p&gt;还是从模型本身出发，P(Y=0|X)=1/(1+exp(-y(w’x))，MLE的目的是让P(Y=1|X)最大，也就是P(Y=0|X)最小：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.19.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;erri=log(1+exp(-ys))，而log(1+exp(-ys))被称为log-loss。&lt;/p&gt;
&lt;p&gt;如果y~N(0,1)，我们将得到probit模型，这是一个和LogReg很相似的模型，同样具有归一化作用。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-简单的线性分类模型&quot;&gt;&lt;a href=&quot;#1-简单的线性分类模型&quot; class=&quot;headerlink&quot; title=&quot;1.简单的线性分类模型&quot;&gt;&lt;/a&gt;1.简单的线性分类模型&lt;/h2&gt;&lt;p&gt;前面介绍的线性模型(LM)适应于回归。一般的线性模型可以用于分类吗？
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>闲谈机器学习(1)</title>
    <link href="http://yoursite.com/2016/04/02/ML1/"/>
    <id>http://yoursite.com/2016/04/02/ML1/</id>
    <published>2016-04-02T13:04:42.000Z</published>
    <updated>2016-05-06T08:43:12.387Z</updated>
    
    <content type="html">&lt;h2 id=&quot;1-OLS：一般最小二乘回归&quot;&gt;&lt;a href=&quot;#1-OLS：一般最小二乘回归&quot; class=&quot;headerlink&quot; title=&quot;1.OLS：一般最小二乘回归&quot;&gt;&lt;/a&gt;1.OLS：一般最小二乘回归&lt;/h2&gt;&lt;p&gt;这是最普通也是最原始的回归模型。在20世纪就由Gauss等数学家提出。OLS的最优特点是有闭式解，甚至不需要太多的优化手段。OLS顾名思义，采用平方损失函数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;那么最优化问题为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;得到：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.4.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;由于有close form的解（不讨论广义逆等情况），所以求解起来非常容易。请注意w*，之后的很多变形和它都有关系。&lt;/p&gt;
&lt;h2 id=&quot;2-Ridge-Regression&quot;&gt;&lt;a href=&quot;#2-Ridge-Regression&quot; class=&quot;headerlink&quot; title=&quot;2.Ridge Regression&quot;&gt;&lt;/a&gt;2.Ridge Regression&lt;/h2&gt;&lt;p&gt;对于预测的w*，有如下定理：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.5.gif&quot;&gt;，这是bias-varience分解的另一种表达。&lt;/p&gt;
&lt;p&gt;由于w是无偏估计（Gauss-Markov），所以：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.6.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;当X’X的特征值非常小时，会出现MSE非常大的情况。所以我们期望对w*的表达式做一改造，使之避免这种情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.7.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;显然，X’X的特征值将同时增大。这也是防止X’X变为奇异矩阵的方法。这会减小varience。&lt;/p&gt;
&lt;p&gt;从机器学习的角度来说，这相当于Regul.，惩罚过大的协方差。使用l2-norm Regul.(Tikhonov Regul.)防止overfitting。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.8.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Ridge Reg的实现和OLS很相似，这里不再赘述。只有一个超参数需要人为调整，可以采用Grid Search或者使用先验知识等方法进行设定。&lt;/p&gt;
&lt;h2 id=&quot;3-Lasso&quot;&gt;&lt;a href=&quot;#3-Lasso&quot; class=&quot;headerlink&quot; title=&quot;3.Lasso&quot;&gt;&lt;/a&gt;3.Lasso&lt;/h2&gt;&lt;p&gt;我们还可以发现，Ridge Reg解决的实际上是数据的特征&amp;gt;数据量(n&amp;gt;m)的问题，由于r(X’X)&amp;lt;=r(X)&amp;lt;=m&amp;lt;n，而X’X是n*n的矩阵。所以减少数据的维数就可以解决。&lt;/p&gt;
&lt;p&gt;Lasso添加l1-norm Regul.罚项，它是一种更趋向于选择稀疏解的线性回归。（具体可以看统计学教材）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.9.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;罚函数为P(w)，我们假设P(w)为向量lp范。p&amp;gt;1，此时P(w)为光滑凸函数；p=1，为不光滑凸函数；0&amp;lt;p&amp;lt;1为非凸函数.&lt;/p&gt;
&lt;p&gt;我们倾向于解决p&amp;gt;1的可微凸优化问题。当p=1时，就得到Lasso。p=2为Ridge Regression。Lasso无法给出一个闭式解。需要采用优化方法。由于Lasso的稀疏性，广泛运用在sparse learning中&lt;/p&gt;
&lt;h2 id=&quot;4-概率视角&quot;&gt;&lt;a href=&quot;#4-概率视角&quot; class=&quot;headerlink&quot; title=&quot;4.概率视角&quot;&gt;&lt;/a&gt;4.概率视角&lt;/h2&gt;&lt;p&gt;以上的理论推导并未用到任何概率知识，仅仅采用最小二乘法，解了一个线性方程组而已。以下采用概率统计的视角的得到相同的结果。&lt;/p&gt;
&lt;p&gt;显然，&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.24.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看出，OLS实际是在Gaussian分布下的MLE。&lt;/p&gt;
&lt;p&gt;我们再进行一些更复杂的推导，可以得出另一个结论。选用共轭先验：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.24.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;Ridge Reg实际是先验为Gaussian的MAP估计。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-OLS：一般最小二乘回归&quot;&gt;&lt;a href=&quot;#1-OLS：一般最小二乘回归&quot; class=&quot;headerlink&quot; title=&quot;1.OLS：一般最小二乘回归&quot;&gt;&lt;/a&gt;1.OLS：一般最小二乘回归&lt;/h2&gt;&lt;p&gt;这是最普通也是最原始的回归模型。在20世纪就
    
    </summary>
    
    
  </entry>
  
</feed>
