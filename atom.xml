<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ClT&#39;s Blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-04-04T11:55:52.604Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ClT</name>
    <email>cl.tian@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2016/04/04/about/"/>
    <id>http://yoursite.com/2016/04/04/about/</id>
    <published>2016-04-04T11:55:52.604Z</published>
    <updated>2016-04-04T11:55:52.604Z</updated>
    
    <content type="html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;p&gt;一根筋，略高冷，技术小白。近三年的目标是静下心来好好学技术。&lt;/p&gt;
&lt;p&gt;博客主要面向机器学习、算法设计等领域，包括私事在内的闲聊也可能会有。&lt;/p&gt;
&lt;p&gt;认真做事，厚积薄发。&lt;/p&gt;
&lt;p&gt;如果有大神指点是最好不过的了～&lt;/p&gt;
&lt;p&gt;email: cl.tian@qq.com&lt;/p&gt;
&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/ali.jpg&quot;&gt;
&lt;/html&gt;
</content>
    
    <summary type="html">
    
      &lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;p&gt;一根筋，略高冷，技术小白。近三年的目标是静下心来好好学技术。&lt;/p&gt;
&lt;p&gt;博客主要面向机器学习、算法设计等领域，包括私事在内的闲聊也可能会有。&lt;/p&gt;
&lt;p&gt;认真做事，厚积薄发。&lt;/p&gt;
&lt;p&gt;如果有大神指点是最好不过的了～&lt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning(2)</title>
    <link href="http://yoursite.com/2016/04/02/ML2/"/>
    <id>http://yoursite.com/2016/04/02/ML2/</id>
    <published>2016-04-02T15:04:05.000Z</published>
    <updated>2016-04-02T15:05:46.942Z</updated>
    
    <content type="html">&lt;h1 id=&quot;机器学习1——线性模型-2&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-2&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(2)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(2)&lt;/h1&gt;&lt;h2 id=&quot;1-形式化的LM&quot;&gt;&lt;a href=&quot;#1-形式化的LM&quot; class=&quot;headerlink&quot; title=&quot;1.形式化的LM&quot;&gt;&lt;/a&gt;1.形式化的LM&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;得分：s=w’x。表示training set中不同属性的加权求和。feature space-&amp;gt;实数空间的映射。&lt;/li&gt;
&lt;li&gt;学习目标：argmin err(out)=err(in)+A=f(s,y)+A。学习目标是最小化输出误差，输出误差=经验误差+泛化能力损失=f(输出，得分）+泛化能力损失&lt;/li&gt;
&lt;li&gt;对OLS来说，采用平方误差，A=0；对Ridge Reg来说，采用平方误差，A=Tikhonov Regularization；对Lasso来说，采用平方误差，A=l1-norm。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;2-简单的线性分类模型&quot;&gt;&lt;a href=&quot;#2-简单的线性分类模型&quot; class=&quot;headerlink&quot; title=&quot;2.简单的线性分类模型&quot;&gt;&lt;/a&gt;2.简单的线性分类模型&lt;/h2&gt;&lt;p&gt;前面介绍的线性模型(LM)适应于回归。一般的线性模型可以用于分类吗？可以有以下朴素模型：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.1.gif&quot;&gt;&lt;br&gt;但是这个优化问题不可导，类似于组合优化，是一个NP-hard问题。&lt;/p&gt;
&lt;p&gt;放宽一下，不要求那么精确，我们只要看y(w’x)是否同号即可：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.2.gif&quot;&gt;&lt;br&gt;这被称为：感知机模型(perceptron)，是最简单的一个线性分类模型，也是最简单的神经网络模型(ANN)。这个问题容易优化，使用SGD（随机梯度下降法）很容易求解。但是这个模型能力非常有限，具体将在ANN部分说明。&lt;/p&gt;
&lt;h2 id=&quot;3-广义线性模型-GLM&quot;&gt;&lt;a href=&quot;#3-广义线性模型-GLM&quot; class=&quot;headerlink&quot; title=&quot;3.广义线性模型(GLM)&quot;&gt;&lt;/a&gt;3.广义线性模型(GLM)&lt;/h2&gt;&lt;p&gt;GLM是LM的推广，在统计学上有很严密的理论支撑。机器学习领域最明显的应用是：logistic回归。&lt;br&gt;这里通过3个角度简单说明LogReg。&lt;/p&gt;
&lt;p&gt;所谓GLM，是指它扩展了OLS的误差正态性假设，假设y服从指数分布族的一个分布（正态分布是指数分布族的一支）。如果y~Bern(p)(y服从伯努利分布），那么：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.3.gif&quot;&gt;&lt;br&gt;当p&amp;lt;=0.5，则y=-1;当p&amp;gt;0.5，则y=+1.&lt;br&gt;这个模型具有很强的扩展性，如果y~Cat(p1,p2,…)时，可以得到用于多分类的LogReg。（这里的具体推导不书写）&lt;/p&gt;
&lt;p&gt;第二个角度更容易理解。我们定义归一化sigmoid函数(softmax)：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.4.gif&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.5.png&quot; width=&quot;500&quot; height=&quot;200&quot;&gt;&lt;br&gt;sigmoid(w’x)相当于将值域压缩到[0,1]。&lt;/p&gt;
&lt;p&gt;并且，sigmoid函数二阶可导，g’(z)=g(z)(1-g(z))，这对优化而言是好事。对LogReg的优化采用MLE+GD/SGD/Newton/quasi-Newton等数值优化方法皆可。使用GD或者Newton法都可以迭代求解。&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.1.gif&quot;&gt;&lt;br&gt;第三个角度仍然从形式化的LM出发。&lt;/p&gt;
&lt;p&gt;从L(w)可以得出：err(in)=log(1+exp(s))-s&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.4.gif&quot;&gt;&lt;br&gt;且&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.5.gif&quot;&gt;&lt;br&gt;可以认为log(1+exp(s))-s=log(1+exp(-s))，而log(1+exp(-s))被称为log-loss。还有一种Loss叫做cross-entropy-loss，在LogReg中，log-loss=cross-entopy-loss。所以LogReg的err(in)=log-loss(s)=cross-entropy-loss(s)&lt;/p&gt;
&lt;p&gt;如果y~N(0,1)，我们将得到probit模型，这是一个和LogReg很相似的模型，同样具有归一化作用。&lt;/p&gt;
&lt;h2 id=&quot;4-Bayesian-LogReg&quot;&gt;&lt;a href=&quot;#4-Bayesian-LogReg&quot; class=&quot;headerlink&quot; title=&quot;4.Bayesian LogReg&quot;&gt;&lt;/a&gt;4.Bayesian LogReg&lt;/h2&gt;&lt;p&gt;y~Bern(p)，我们找不到一个优良的先验分布，但又不想使用uninformative prior。这个时候使用一种新的技巧：appropriation来做。由于Gaussian有良好的congjugate prior，Bern和Gaussian都属于指数分布族，我们考虑使用Gaussian去近似Bern分布。这个方法叫Laplace appropriation。&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.3.gif&quot;&gt;&lt;br&gt;具体不再深入。（可见PRML和MLPP）&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习1——线性模型-2&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-2&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(2)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(2)&lt;/h1&gt;&lt;h2 id=&quot;1-形式化的LM&quot;&gt;&lt;a href=&quot;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning(1)</title>
    <link href="http://yoursite.com/2016/04/02/ML1/"/>
    <id>http://yoursite.com/2016/04/02/ML1/</id>
    <published>2016-04-02T13:04:42.000Z</published>
    <updated>2016-04-02T14:29:40.818Z</updated>
    
    <content type="html">&lt;h1 id=&quot;机器学习1——线性模型-1&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-1&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(1)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(1)&lt;/h1&gt;&lt;h2 id=&quot;1-为什么选择线性模型？&quot;&gt;&lt;a href=&quot;#1-为什么选择线性模型？&quot; class=&quot;headerlink&quot; title=&quot;1.为什么选择线性模型？&quot;&gt;&lt;/a&gt;1.为什么选择线性模型？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;线性模型(LM)简单。线性模型的参数较少，直观，易于优化计算。&lt;/li&gt;
&lt;li&gt;抗overfitting的效果好。较少的参数意味着模型的泛化能力很强，可以有效的防止overfitting。符合Occam’s Razor原理。&lt;/li&gt;
&lt;li&gt;数学基础坚实。基于统计学和优化计算。&lt;/li&gt;
&lt;li&gt;扩展能力强。一般的线性模型可以轻易的扩展为GLM，ridge LinReg（岭回归），Lasso，SVM。由表示定理，可以kernel化，进而进行非线性分类和回归。继续扩展，还有Bayesian LinReg和Bayesian LogReg和Gaussian Processes回归等。可以说LM是机器学习领域涵盖面非常广的一大类“好”模型。实用且高效。&lt;/li&gt;
&lt;li&gt;LM主要包括：回归(Reg)模型和SVM模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;2-OLS：一般最小二乘回归&quot;&gt;&lt;a href=&quot;#2-OLS：一般最小二乘回归&quot; class=&quot;headerlink&quot; title=&quot;2.OLS：一般最小二乘回归&quot;&gt;&lt;/a&gt;2.OLS：一般最小二乘回归&lt;/h2&gt;&lt;p&gt;这是最普通也是最原始的回归模型。在20世纪就由Gauss等数学家提出。OLS的最优特点是有闭式解，甚至不需要太多的优化手段。OLS顾名思义，采用平方损失函数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;那么最优化问题为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;得到：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.4.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;由于有close form的解（不讨论广义逆等情况），所以求解起来非常容易。请注意w*，之后的很多变形和它都有关系。&lt;/p&gt;
&lt;h2 id=&quot;3-Ridge-Regression&quot;&gt;&lt;a href=&quot;#3-Ridge-Regression&quot; class=&quot;headerlink&quot; title=&quot;3.Ridge Regression&quot;&gt;&lt;/a&gt;3.Ridge Regression&lt;/h2&gt;&lt;p&gt;对于预测的w*，有如下定理：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.5.gif&quot;&gt;，这是bias-varience分解的另一种表达。&lt;/p&gt;
&lt;p&gt;由于w是无偏估计（Gauss-Markov），所以：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.6.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;当X’X的特征值非常小时，会出现MSE非常大的情况。所以我们期望对w*的表达式做一改造，使之避免这种情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.7.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;显然，X’X的特征值将同时增大。这也是防止X’X变为奇异矩阵的方法。这会减小varience。&lt;/p&gt;
&lt;p&gt;从机器学习的角度来说，这相当于regularization，惩罚过大的协方差。使用l2-norms(Tikhonov Regularization)防止overfitting。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.8.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Ridge Reg的实现和OLS很相似，这里不再赘述。只有一个超参数需要人为调整，可以采用Grid Search或者使用先验知识等方法进行设定。&lt;/p&gt;
&lt;h2 id=&quot;4-Lasso&quot;&gt;&lt;a href=&quot;#4-Lasso&quot; class=&quot;headerlink&quot; title=&quot;4.Lasso&quot;&gt;&lt;/a&gt;4.Lasso&lt;/h2&gt;&lt;p&gt;我们还可以发现，Ridge Reg解决的实际上是数据的特征&amp;gt;数据量(n&amp;gt;m)的问题，由于r(X’X)&amp;lt;=r(X)&amp;lt;=m&amp;lt;n，而X’X是n*n的矩阵。所以减少数据的维数就可以解决。&lt;/p&gt;
&lt;p&gt;Lasso是一种更趋向于选择稀疏解的线性回归。（具体可以看统计学教材）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.9.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;罚函数为P(w)，我们假设P(w)为向量lp范。p&amp;gt;1，此时P(w)为光滑凸函数；p=1，为不光滑凸函数；0&amp;lt;p&amp;lt;1为非凸函数.&lt;/p&gt;
&lt;p&gt;我们倾向于解决p&amp;gt;1的可微凸优化问题。当p=1时，就得到Lasso。p=2为Ridge Regression。Lasso无法给出一个闭式解。需要采用优化方法。（具体的优化手段我并不清楚）。Lasso的思想已经延伸到稀疏学习、压缩感知等领域。&lt;/p&gt;
&lt;p&gt;和Lasso相似的还有逐步回归，采用Greedy策略，也可以达到一定意义上的稀疏解的效果。这些方法和特征选择以及降维都有所联系。&lt;/p&gt;
&lt;h2 id=&quot;5-Baysian-LinReg&quot;&gt;&lt;a href=&quot;#5-Baysian-LinReg&quot; class=&quot;headerlink&quot; title=&quot;5.Baysian LinReg&quot;&gt;&lt;/a&gt;5.Baysian LinReg&lt;/h2&gt;&lt;p&gt;Bayesian LinReg是将线性回归放在贝叶斯统计的语义下推出的一种回归。看起来OLS的推理并没有用到统计，实际上，可以看到y服从高斯分布。前面最小平方损失的优化过程实际也可以看做是最大似然（MLE）估计参数的过程。&lt;/p&gt;
&lt;p&gt;在贝叶斯语义下，我们假设参数分为2种：参数parameter和超参数hyperparameter。其中parameter是随机变量(r.v.)，hyperparameter是一般的变量（或者是uninformative prior)。parameter的prior分布p.d.f.参数为hyperparameter。观察得到的是likelihood。通过全概率和贝叶斯公式，我们可以得到posterior分布（参数的后验分布）。我们需要做的是使用type-II MLE估计参数（或者使用其他方法得到参数后验的值）。由于Bayesian Occam’s Razor原理，Bayesian LinReg会有更好的泛化能力。&lt;/p&gt;
&lt;p&gt;先验的一般选择或者共轭先验(conjugate prior)，我们需要注意的是evidence服从正态，协方差不予考虑，此时参数的的先验也是正态。（形式化表达参见”Machine Learning - A Probabilistic Perspective “ 7.6）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.2.png&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习1——线性模型-1&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-1&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(1)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(1)&lt;/h1&gt;&lt;h2 id=&quot;1-为什么选择线性模型？&quot;&gt;&lt;a hr
    
    </summary>
    
    
  </entry>
  
</feed>
