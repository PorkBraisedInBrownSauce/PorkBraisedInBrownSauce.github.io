<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ClT&#39;s Blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-04-10T09:59:22.870Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ClT</name>
    <email>cl.tian@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习(4)</title>
    <link href="http://yoursite.com/2016/04/09/ML4/"/>
    <id>http://yoursite.com/2016/04/09/ML4/</id>
    <published>2016-04-09T09:46:05.000Z</published>
    <updated>2016-04-10T09:59:22.870Z</updated>
    
    <content type="html">&lt;h1 id=&quot;机器学习1——线性模型-4&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-4&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(4)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(4)&lt;/h1&gt;&lt;h2 id=&quot;1-kernel-Ridge-Regression&quot;&gt;&lt;a href=&quot;#1-kernel-Ridge-Regression&quot; class=&quot;headerlink&quot; title=&quot;1.kernel Ridge Regression&quot;&gt;&lt;/a&gt;1.kernel Ridge Regression&lt;/h2&gt;&lt;p&gt;将表示定理用于Ridge Regression，可以将线性Ridge Regression推广到非线性。再叙述一次表示定理：&lt;/p&gt;
&lt;p&gt;简单来说，一个优化问题表示为：argmin err(out)=l[(x1,y1,f(x1),…,(xn,yn,f(xn)]+g(||f||) &lt;/p&gt;
&lt;p&gt;此时，l(.)非负，g(.)为单调增函数，那么存在满足Mercer定理的核函数，使得解总可以写作：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;对于l2-norm regularization LM：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.1.gif&quot;&gt;&lt;br&gt;我们将看到，这个定理的强大作用。它使LM变成了非线性模型，而不需要对LM的框架更改许多。&lt;/p&gt;
&lt;p&gt;Ridge Reg可以表示为:argmin 平方损失+l2-norm regularization。完全符合表示定理。则：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;使用矩阵表示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;解得&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.4.gif&quot;&gt;&lt;br&gt;但是这个矩阵比较dense，数值解会浪费时间，且解不稳定。所以实务中用的不多。&lt;/p&gt;
&lt;h2 id=&quot;2-kernel-l2-norm-regularized-LogReg&quot;&gt;&lt;a href=&quot;#2-kernel-l2-norm-regularized-LogReg&quot; class=&quot;headerlink&quot; title=&quot;2.kernel l2-norm regularized LogReg&quot;&gt;&lt;/a&gt;2.kernel l2-norm regularized LogReg&lt;/h2&gt;&lt;p&gt;将表示定理用于l2-norm regularized LogReg，可以将线性Ridge Regression推广到非线性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个问题不像SVM具有稀疏性，实务中用的也不多。&lt;/p&gt;
&lt;h2 id=&quot;3-Probabilistic-SVM&quot;&gt;&lt;a href=&quot;#3-Probabilistic-SVM&quot; class=&quot;headerlink&quot; title=&quot;3.Probabilistic SVM&quot;&gt;&lt;/a&gt;3.Probabilistic SVM&lt;/h2&gt;&lt;p&gt;继续研究LogReg和SVM。&lt;/p&gt;
&lt;p&gt;soft-margin SVM使用hinge loss，LogReg使用log loss。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.6.jpg&quot; width=&quot;500&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看到log-loss和hinge-loss很相近，实际上SVM约等于l2-norm regularized LogReg。通常情况下，他们的性能也相当。LogReg主要优势在于可以直接解释为概率。SVM的主要优势在于SVM的解具有稀疏性，因而需要的样本也更少，开销小。&lt;/p&gt;
&lt;p&gt;我们可以将SVM和LogReg结合起来。先使用SVM得到一个线性模型。然后将转换后的数据带入LogReg，SVM中可以使用kernel，这个模型同样可以实现非线性分类。这就是所谓的2-stage model。叫做probabilistic SVM。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.7.gif&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;4-SVR&quot;&gt;&lt;a href=&quot;#4-SVR&quot; class=&quot;headerlink&quot; title=&quot;4.SVR&quot;&gt;&lt;/a&gt;4.SVR&lt;/h2&gt;&lt;p&gt;将SVM的思想推广到回归就得到了SVR。让回归曲线也尽可能“胖”一点。仿照soft-margin SVM：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.9.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;按照SVM的思路，可以导出SVR，同样可以kernel化。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习1——线性模型-4&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-4&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(4)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(4)&lt;/h1&gt;&lt;h2 id=&quot;1-kernel-Ridge-Regr
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习(3)</title>
    <link href="http://yoursite.com/2016/04/09/ML3/"/>
    <id>http://yoursite.com/2016/04/09/ML3/</id>
    <published>2016-04-09T09:45:15.000Z</published>
    <updated>2016-04-10T09:59:22.870Z</updated>
    
    <content type="html">&lt;h1 id=&quot;机器学习1——线性模型-3&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-3&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(3)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(3)&lt;/h1&gt;&lt;h2 id=&quot;1-hard-margin-SVM&quot;&gt;&lt;a href=&quot;#1-hard-margin-SVM&quot; class=&quot;headerlink&quot; title=&quot;1.hard-margin SVM&quot;&gt;&lt;/a&gt;1.hard-margin SVM&lt;/h2&gt;&lt;p&gt;支持向量模型是第二大类线性模型，它着眼于直接增强模型的泛化能力。即让分界面尽可能的“胖”。SVM用于分类。&lt;/p&gt;
&lt;p&gt;如果分类器能正确分类：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;一个点在n维空间到分类超平面的距离：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;取分子为边界值：（即hard-margin）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时，我们得到一个有约束的优化问题：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.4.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个问题不好优化，把它变成一个好优化的问题（等价变换）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这是一个二次优化问题。（凸问题是最愿意见到的一个问题）对比具有hard-margin和没有margin的情形：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.6.png&quot;&gt;   &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.7.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;可见分类的鲁棒性大大增强。&lt;/p&gt;
&lt;h2 id=&quot;2-soft-margin-SVM&quot;&gt;&lt;a href=&quot;#2-soft-margin-SVM&quot; class=&quot;headerlink&quot; title=&quot;2.soft-margin SVM&quot;&gt;&lt;/a&gt;2.soft-margin SVM&lt;/h2&gt;&lt;p&gt;在前面的讨论中，我们一直假定训练样本是线性可分的，然而，现实中很多数据不是线性可分的，这个时候要进一步提升模型的泛化能力。使它也能容忍一部分线性不可分的training data。&lt;/p&gt;
&lt;p&gt;采用增加正则化项的方法：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.8.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;后面那部分就是正则化项（罚函数），也就是优化问题会同时考虑margin最大和误分类点尽可能少。但0/1损失数学性质不好，我们改为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.9.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中max(0,1-z)叫做hinge损失函数。如果换一个看法：err(out)=hinge(s)+l2-norm Regularization。又回到了经典的LM那里。&lt;/p&gt;
&lt;p&gt;换第三个看法：引入松弛变量，将式子再改写一下&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.10.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;注意到和hard-margin SVM 相比，soft-margin SVM多了松弛变量，正是一个松弛变量让margin可以自动调整。&lt;/p&gt;
&lt;h2 id=&quot;3-Dual问题&quot;&gt;&lt;a href=&quot;#3-Dual问题&quot; class=&quot;headerlink&quot; title=&quot;3.Dual问题&quot;&gt;&lt;/a&gt;3.Dual问题&lt;/h2&gt;&lt;p&gt;基于优化的primal-dual理论，我们可以得到hard-margin SVM和soft-margin SVM的dual问题。&lt;/p&gt;
&lt;p&gt;dual hard-margin SVM:&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.11.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;dual soft-margin SVM:&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.5.12.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;由对偶松弛条件，dual问题有较为稀疏的解，这是我们愿意看到的。&lt;/p&gt;
&lt;h2 id=&quot;4-kernel-trick&quot;&gt;&lt;a href=&quot;#4-kernel-trick&quot; class=&quot;headerlink&quot; title=&quot;4.kernel trick&quot;&gt;&lt;/a&gt;4.kernel trick&lt;/h2&gt;&lt;p&gt;kernel在数学上是一个很宽泛的概念，这里的kernel特指能够隐形的进行内积运算的函数：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.1.gif&quot;&gt;&lt;br&gt;试想，如果没有kernel，进行上述运算，运算复杂度为平方量级，如果引入kernel，会变成线性量级。这个数学概念在机器学习中有什么用呢？&lt;/p&gt;
&lt;p&gt;请观察dual hard-margin SVM和dual soft-margin SVM，里面含有内积运算。但是他们仍然只能进行线性分类，如果我们有一种映射，将低维不可分映射到足够高维，可能就会成为高维的线性可分问题。&lt;/p&gt;
&lt;p&gt;此时dual soft-margin SVM:&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;使用kernel:&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;这样线性分类的SVM自然地扩展到了非线性。&lt;/p&gt;
&lt;p&gt;再深入讨论。只有核矩阵是半正定（半正定Gram矩阵）的kernel才是可用的kernel。事实上，对于一个半正定的核矩阵，总能找到一个与之对应的映射。任何一个kernel都隐式定义了一个再生希尔伯特核空间(RKHS)。（不需要理解什么是RKHS，这是一个泛函空间，并满足了一定性质）即Mercer定理。&lt;/p&gt;
&lt;p&gt;数学推导得到了重要的&lt;strong&gt;表示定理&lt;/strong&gt;：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.4.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;简单来说，一个优化问题表示为：argmin err(out)=l[(x1,y1,f(x1),…,(xn,yn,f(xn)]+g(||f||) &lt;/p&gt;
&lt;p&gt;此时，l(.)非负，g(.)为单调增函数，那么存在满足Mercer定理的核函数，使得解总可以写作：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.6.5.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;对于l2-norm regularization LM：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.7.1.gif&quot;&gt;&lt;br&gt;我们将看到，这个定理的强大作用。它使LM变成了非线性模型，而不需要对LM的框架更改许多。&lt;/p&gt;
&lt;p&gt;将SVM的dual很容易推广到非线性情形。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习1——线性模型-3&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-3&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(3)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(3)&lt;/h1&gt;&lt;h2 id=&quot;1-hard-margin-SVM&quot;&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习(2)</title>
    <link href="http://yoursite.com/2016/04/02/ML2/"/>
    <id>http://yoursite.com/2016/04/02/ML2/</id>
    <published>2016-04-02T15:04:05.000Z</published>
    <updated>2016-04-10T09:59:22.870Z</updated>
    
    <content type="html">&lt;h1 id=&quot;机器学习1——线性模型-2&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-2&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(2)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(2)&lt;/h1&gt;&lt;h2 id=&quot;1-形式化的LM&quot;&gt;&lt;a href=&quot;#1-形式化的LM&quot; class=&quot;headerlink&quot; title=&quot;1.形式化的LM&quot;&gt;&lt;/a&gt;1.形式化的LM&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;得分：s=w’x。表示training set中不同属性的加权求和。feature space-&amp;gt;实数空间的映射。&lt;/li&gt;
&lt;li&gt;学习目标：argmin err(out)=err(in)+A=f(s,y)+A。学习目标是最小化输出误差，输出误差=经验误差+泛化能力损失=f(输出，得分）+泛化能力损失&lt;/li&gt;
&lt;li&gt;对OLS来说，采用平方误差，A=0；对Ridge Reg来说，采用平方误差，A=Tikhonov Regularization；对Lasso来说，采用平方误差，A=l1-norm。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;2-简单的线性分类模型&quot;&gt;&lt;a href=&quot;#2-简单的线性分类模型&quot; class=&quot;headerlink&quot; title=&quot;2.简单的线性分类模型&quot;&gt;&lt;/a&gt;2.简单的线性分类模型&lt;/h2&gt;&lt;p&gt;前面介绍的线性模型(LM)适应于回归。一般的线性模型可以用于分类吗？可以有以下朴素模型：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.1.gif&quot;&gt;&lt;br&gt;但是这个优化问题不可导，类似于组合优化，是一个NP-hard问题。&lt;/p&gt;
&lt;p&gt;放宽一下，不要求那么精确，我们只要看y(w’x)是否同号即可：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.2.gif&quot;&gt;&lt;br&gt;这被称为：感知机模型(perceptron)，是最简单的一个线性分类模型，也是最简单的神经网络模型(ANN)。这个问题容易优化，使用SGD（随机梯度下降法）很容易求解。但是这个模型能力非常有限，具体将在ANN部分说明。&lt;/p&gt;
&lt;h2 id=&quot;3-广义线性模型-GLM&quot;&gt;&lt;a href=&quot;#3-广义线性模型-GLM&quot; class=&quot;headerlink&quot; title=&quot;3.广义线性模型(GLM)&quot;&gt;&lt;/a&gt;3.广义线性模型(GLM)&lt;/h2&gt;&lt;p&gt;GLM是LM的推广，在统计学上有很严密的理论支撑。机器学习领域最明显的应用是：logistic回归。&lt;br&gt;这里通过3个角度简单说明LogReg。&lt;/p&gt;
&lt;p&gt;所谓GLM，是指它扩展了OLS的误差正态性假设，假设y服从指数分布族的一个分布（正态分布是指数分布族的一支）。如果y~Bern(p)(y服从伯努利分布），那么：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.3.gif&quot;&gt;&lt;br&gt;当p&amp;lt;=0.5，则y=-1;当p&amp;gt;0.5，则y=+1.&lt;br&gt;这个模型具有很强的扩展性，如果y~Cat(p1,p2,…)时，可以得到用于多分类的LogReg。（这里的具体推导不书写）&lt;/p&gt;
&lt;p&gt;第二个角度更容易理解。我们定义归一化sigmoid函数(softmax)：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.4.gif&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.31.5.png&quot; width=&quot;500&quot; height=&quot;200&quot;&gt;&lt;br&gt;sigmoid(w’x)相当于将值域压缩到[0,1]。&lt;/p&gt;
&lt;p&gt;并且，sigmoid函数二阶可导，g’(z)=g(z)(1-g(z))，这对优化而言是好事。对LogReg的优化采用MLE+GD/SGD/Newton/quasi-Newton等数值优化方法皆可。使用GD或者Newton法都可以迭代求解。&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.1.gif&quot;&gt;&lt;br&gt;第三个角度仍然从形式化的LM出发。&lt;/p&gt;
&lt;p&gt;从L(w)可以得出：err(in)=log(1+exp(s))-s&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.4.gif&quot;&gt;&lt;br&gt;且&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.5.gif&quot;&gt;&lt;br&gt;可以认为log(1+exp(s))-s=log(1+exp(-s))，而log(1+exp(-s))被称为log-loss。还有一种Loss叫做cross-entropy-loss，在LogReg中，log-loss=cross-entopy-loss。所以LogReg的err(in)=log-loss(s)=cross-entropy-loss(s)&lt;/p&gt;
&lt;p&gt;如果y~N(0,1)，我们将得到probit模型，这是一个和LogReg很相似的模型，同样具有归一化作用。&lt;/p&gt;
&lt;h2 id=&quot;4-Bayesian-LogReg&quot;&gt;&lt;a href=&quot;#4-Bayesian-LogReg&quot; class=&quot;headerlink&quot; title=&quot;4.Bayesian LogReg&quot;&gt;&lt;/a&gt;4.Bayesian LogReg&lt;/h2&gt;&lt;p&gt;y~Bern(p)，我们找不到一个优良的先验分布，但又不想使用uninformative prior。这个时候使用一种新的技巧：appropriation来做。由于Gaussian有良好的congjugate prior，Bern和Gaussian都属于指数分布族，我们考虑使用Gaussian去近似Bern分布。这个方法叫Laplace appropriation。&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.3.gif&quot;&gt;&lt;br&gt;具体不再深入。（可见PRML和MLPP）&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习1——线性模型-2&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-2&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(2)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(2)&lt;/h1&gt;&lt;h2 id=&quot;1-形式化的LM&quot;&gt;&lt;a href=&quot;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习(1)</title>
    <link href="http://yoursite.com/2016/04/02/ML1/"/>
    <id>http://yoursite.com/2016/04/02/ML1/</id>
    <published>2016-04-02T13:04:42.000Z</published>
    <updated>2016-04-10T09:59:22.854Z</updated>
    
    <content type="html">&lt;h1 id=&quot;机器学习1——线性模型-1&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-1&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(1)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(1)&lt;/h1&gt;&lt;h2 id=&quot;1-为什么选择线性模型？&quot;&gt;&lt;a href=&quot;#1-为什么选择线性模型？&quot; class=&quot;headerlink&quot; title=&quot;1.为什么选择线性模型？&quot;&gt;&lt;/a&gt;1.为什么选择线性模型？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;线性模型(LM)简单。线性模型的参数较少，直观，易于优化计算。&lt;/li&gt;
&lt;li&gt;抗overfitting的效果好。较少的参数意味着模型的泛化能力很强，可以有效的防止overfitting。符合Occam’s Razor原理。&lt;/li&gt;
&lt;li&gt;数学基础坚实。基于统计学和优化计算。&lt;/li&gt;
&lt;li&gt;扩展能力强。一般的线性模型可以轻易的扩展为GLM，ridge LinReg（岭回归），Lasso，SVM。由表示定理，可以kernel化，进而进行非线性分类和回归。继续扩展，还有Bayesian LinReg和Bayesian LogReg和Gaussian Processes回归等。可以说LM是机器学习领域涵盖面非常广的一大类“好”模型。实用且高效。&lt;/li&gt;
&lt;li&gt;LM主要包括：回归(Reg)模型和SVM模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;2-OLS：一般最小二乘回归&quot;&gt;&lt;a href=&quot;#2-OLS：一般最小二乘回归&quot; class=&quot;headerlink&quot; title=&quot;2.OLS：一般最小二乘回归&quot;&gt;&lt;/a&gt;2.OLS：一般最小二乘回归&lt;/h2&gt;&lt;p&gt;这是最普通也是最原始的回归模型。在20世纪就由Gauss等数学家提出。OLS的最优特点是有闭式解，甚至不需要太多的优化手段。OLS顾名思义，采用平方损失函数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.1.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;那么最优化问题为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.2.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;得到：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.3.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.4.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;由于有close form的解（不讨论广义逆等情况），所以求解起来非常容易。请注意w*，之后的很多变形和它都有关系。&lt;/p&gt;
&lt;h2 id=&quot;3-Ridge-Regression&quot;&gt;&lt;a href=&quot;#3-Ridge-Regression&quot; class=&quot;headerlink&quot; title=&quot;3.Ridge Regression&quot;&gt;&lt;/a&gt;3.Ridge Regression&lt;/h2&gt;&lt;p&gt;对于预测的w*，有如下定理：&lt;br&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.5.gif&quot;&gt;，这是bias-varience分解的另一种表达。&lt;/p&gt;
&lt;p&gt;由于w是无偏估计（Gauss-Markov），所以：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.6.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;当X’X的特征值非常小时，会出现MSE非常大的情况。所以我们期望对w*的表达式做一改造，使之避免这种情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.7.gif&quot;&gt;&lt;/p&gt;
&lt;p&gt;显然，X’X的特征值将同时增大。这也是防止X’X变为奇异矩阵的方法。这会减小varience。&lt;/p&gt;
&lt;p&gt;从机器学习的角度来说，这相当于regularization，惩罚过大的协方差。使用l2-norms(Tikhonov Regularization)防止overfitting。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.8.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Ridge Reg的实现和OLS很相似，这里不再赘述。只有一个超参数需要人为调整，可以采用Grid Search或者使用先验知识等方法进行设定。&lt;/p&gt;
&lt;h2 id=&quot;4-Lasso&quot;&gt;&lt;a href=&quot;#4-Lasso&quot; class=&quot;headerlink&quot; title=&quot;4.Lasso&quot;&gt;&lt;/a&gt;4.Lasso&lt;/h2&gt;&lt;p&gt;我们还可以发现，Ridge Reg解决的实际上是数据的特征&amp;gt;数据量(n&amp;gt;m)的问题，由于r(X’X)&amp;lt;=r(X)&amp;lt;=m&amp;lt;n，而X’X是n*n的矩阵。所以减少数据的维数就可以解决。&lt;/p&gt;
&lt;p&gt;Lasso是一种更趋向于选择稀疏解的线性回归。（具体可以看统计学教材）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/3.23.9.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;罚函数为P(w)，我们假设P(w)为向量lp范。p&amp;gt;1，此时P(w)为光滑凸函数；p=1，为不光滑凸函数；0&amp;lt;p&amp;lt;1为非凸函数.&lt;/p&gt;
&lt;p&gt;我们倾向于解决p&amp;gt;1的可微凸优化问题。当p=1时，就得到Lasso。p=2为Ridge Regression。Lasso无法给出一个闭式解。需要采用优化方法。Lasso的思想已经延伸到稀疏学习、压缩感知等领域。&lt;/p&gt;
&lt;p&gt;和Lasso相似的还有逐步回归，采用Greedy策略，也可以达到一定意义上的稀疏解的效果。这些方法和特征选择以及降维都有所联系。&lt;/p&gt;
&lt;h2 id=&quot;5-Baysian-LinReg&quot;&gt;&lt;a href=&quot;#5-Baysian-LinReg&quot; class=&quot;headerlink&quot; title=&quot;5.Baysian LinReg&quot;&gt;&lt;/a&gt;5.Baysian LinReg&lt;/h2&gt;&lt;p&gt;Bayesian LinReg是将线性回归放在贝叶斯统计的语义下推出的一种回归。看起来OLS的推理并没有用到统计，实际上，可以看到y服从高斯分布。前面最小平方损失的优化过程实际也可以看做是最大似然（MLE）估计参数的过程。&lt;/p&gt;
&lt;p&gt;在贝叶斯语义下，我们假设参数分为2种：参数parameter和超参数hyperparameter。其中parameter是随机变量(r.v.)，hyperparameter是一般的变量（或者是uninformative prior)。parameter的prior分布p.d.f.参数为hyperparameter。观察得到的是likelihood。通过全概率和贝叶斯公式，我们可以得到posterior分布（参数的后验分布）。我们需要做的是使用type-II MLE估计参数（或者使用其他方法得到参数后验的值）。由于Bayesian Occam’s Razor原理，Bayesian LinReg会有更好的泛化能力。&lt;/p&gt;
&lt;p&gt;先验的一般选择或者共轭先验(conjugate prior)，我们需要注意的是evidence服从正态，协方差不予考虑，此时参数的的先验也是正态。（形式化表达参见MLPP 7.6）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xs6jl.com1.z0.glb.clouddn.com/4.1.2.png&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习1——线性模型-1&quot;&gt;&lt;a href=&quot;#机器学习1——线性模型-1&quot; class=&quot;headerlink&quot; title=&quot;机器学习1——线性模型(1)&quot;&gt;&lt;/a&gt;机器学习1——线性模型(1)&lt;/h1&gt;&lt;h2 id=&quot;1-为什么选择线性模型？&quot;&gt;&lt;a hr
    
    </summary>
    
    
  </entry>
  
</feed>
